{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Implementation With LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG is a technique for augmenting LLM knowledge with additional data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics, but their knowledge is <i> to the public data up to a specific point in time that they were trained on </i>. If you want to build AI applications that can reason about private data or data introduced after a model’s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as <b> Retrieval Augmented Generation (RAG) </b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. It enables applications that:\n",
    "\n",
    "Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)\n",
    "Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a simple Chat Bot using Lang Chain\n",
    "\n",
    "We will rely heavily on the LangChain library to bring together the different components needed for the chatbot.\n",
    "\n",
    "<b>Step-1</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command once to set up the OpenAI key as enviornment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the openAI key\n",
    "import getpass\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise Chat GPT 3.5 object to be used for generating responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE : You need an API Key from OpenAI to use this functionality\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Creating an OpenAI object\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    "llm_chat = ChatOpenAI(\n",
    "    temperature = 0,\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step -2 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chats with *OpenAI's gpt-3.5-turbo and gpt-4 chat models* are typically structured (in plain text) like this:\n",
    "\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "User: Hi AI, how are you today?\n",
    "\n",
    "Assistant: I'm great thank you. How can I help you?\n",
    "\n",
    "User: I'd like to understand string theory.\n",
    "\n",
    "Assistant:\n",
    "The final \"Assistant:\" without a response is what would prompt the model to continue the conversation. In the official OpenAI ChatCompletion endpoint these would be passed to the model in a format like:\n",
    "\n",
    "\n",
    "\n",
    "[\n",
    "    \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
    "    \n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n",
    "\n",
    "]\n",
    "\n",
    "LangChain uses a slightly different format. The message objects like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "   \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step -3 </b>\n",
    "Send message to ChatGPT to get a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! As an AI, I don't have feelings, but I'm here and ready to assist you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "response = llm_chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because response is just another AIMessage object, we can append it to messages, add another HumanMessage, and generate the next response in the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia Day is a national public holiday in Australia that is celebrated annually on January 26th. It commemorates the arrival of the First Fleet of British ships in 1788, which marked the beginning of European settlement in Australia.\n",
      "\n",
      "Australia Day is often celebrated with various events and activities, including community gatherings, barbecues, fireworks, concerts, and citizenship ceremonies. It is a day for Australians to come together and reflect on the country's history, culture, and achievements.\n",
      "\n",
      "However, it is important to note that Australia Day is also a day of controversy and debate. For some Indigenous Australians, it represents the invasion and colonization of their lands, and they refer to it as \"Invasion Day\" or \"Survival Day.\" There have been ongoing discussions about changing the date of Australia Day to a more inclusive day that acknowledges the history and culture of Indigenous Australians.\n",
      "\n",
      "Overall, Australia Day is a significant day in the Australian calendar, but it is also a time for reflection and discussion about the country's past and future.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(response)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"I would like to know about Australia day\")\n",
    "\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "response = llm_chat.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now changing persona of ChatGPT so it will translate everything in Urdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبا! میں بہترین ہوں، شکریہ۔ آپ کیسے ہیں؟\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates from english to urdu.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "]\n",
    "\n",
    "response = llm_chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "آسٹریلیا ڈے کے بارے میں معلومات درکار ہے۔\n",
      "\n",
      "آسٹریلیا ڈے 26 جنوری کو منایا جاتا ہے اور یہ آسٹریلیا کا قومی دن ہے۔ یہ دن آسٹریلیا کے بنیادی تشکیلیں کو یاد کرنے اور منانے کا موقع فراہم کرتا ہے۔\n",
      "\n",
      "اس دن کو مختلف طریقوں سے منایا جاتا ہے جو آسٹریلیا کی تاریخ، ثقافت اور تراث کو نمایاں کرتے ہیں۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منایا جاتا ہے۔ یہ دن عوامی تقریبات، جشنوں، میلوں، موسیقی کے اجراءت اور آتش بازی کے ساتھ منایا جاتا ہے۔\n",
      "\n",
      "آسٹریلیا ڈے کو مختلف طریقوں سے منای\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(response)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"I would like to know about Australia day\")\n",
    "\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "response = llm_chat.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step -4 Dealing with Hallucinations </b>\n",
    "\n",
    "The knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the parametric knowledge of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "The result of this is very clear when we ask LLMs about more recent information, like about the new (and very popular) Llama 2 LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 is a fictional character from the video game \"Fortnite.\" It is a part of the Llama Crew Set and was introduced in Season 3 of the game. Llama 2 is a variation of the original Llama character, known for its vibrant colors and unique design.\n",
      "\n",
      "In the game, Llama 2 is often found as a loot box that players can open to obtain various items such as weapons, resources, and materials. It is highly sought after by players due to the valuable loot it contains.\n",
      "\n",
      "Llama 2 has become an iconic symbol in the Fortnite community and is often associated with luck and good fortune. Its appearance in the game is always a cause for excitement among players, as it can significantly enhance their chances of success.\n",
      "\n",
      "Overall, Llama 2 is a beloved character in Fortnite, known for its distinctive appearance and the valuable loot it provides to players.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"I want to know about Llama 2\"),\n",
    "]\n",
    "\n",
    "# send to chat-gpt\n",
    "response = llm_chat.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle this issue, we feeding knowledge into LLMs in another way. It is called source knowledge and it refers to any information fed into the LLM via the prompt. We can try that with the Llama 2 question. We can take a description of this object from the Llama 2 source page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_information = [\n",
    "    \"Code Llama is a code generation model built on Llama 2, trained on 500B tokens of code. It supports common programming languages being used today, including Python, C++, Java, PHP, Typescript (Javascript), C#, and Bash.\",\n",
    "    \"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llama2_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you tell me about the llama 2 ?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed this additional information to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) developed and released by the creators. These models range in scale from 7 billion to 70 billion parameters. One specific variant of the fine-tuned LLMs is called Llama 2-Chat, which is optimized for dialogue use cases. The Llama 2 models have been found to outperform open-source chat models on most benchmarks tested. Additionally, based on human evaluations for helpfulness and safety, Llama 2-Chat may be considered a suitable substitute for closed-source models. The creators have provided a detailed description of their approach to fine-tuning and safety improvements of Llama 2-Chat, with the intention of enabling the community to build on their work and contribute to the responsible development of large language models.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = llm_chat.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building RAG Chatbots with LangChain\n",
    "\n",
    "In this example, we will build an AI chatbot from start-to-finish so that it can answer automatically about Llama 2 instead of providing the information manually. We will be using LangChain,HuggingFace embeddings, OpenAI, and vector DB, to build a chatbot capable of learning from the external world using Retrieval Augmented Generation (RAG).\n",
    "\n",
    "We will use two techniques to build our chatbot:\n",
    "\n",
    "1- Scrap a dataset sourced from the Llama 2 ArXiv paper and other related papers to help our chatbot answer questions about the latest and greatest in the world of GenAI.\n",
    "\n",
    "2- Scrap multiple webpages to help our chatbot answer questions about the latest and greatest in the world of GenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot (no RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be relying heavily on the LangChain library to bring together the different components needed for our chatbot. To begin, we'll create a simple chatbot without any retrieval augmentation. We do this by initializing a ChatOpenAI object. For this we do need an OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the openAI key\n",
    "import getpass\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Creating an OpenAI object\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chats with *OpenAI's gpt-3.5-turbo and gpt-4 chat models* are typically structured (in plain text) like this:\n",
    "\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "User: Hi AI, how are you today?\n",
    "\n",
    "Assistant: I'm great thank you. How can I help you?\n",
    "\n",
    "User: I'd like to understand string theory.\n",
    "\n",
    "Assistant:\n",
    "The final \"Assistant:\" without a response is what would prompt the model to continue the conversation. In the official OpenAI ChatCompletion endpoint these would be passed to the model in a format like:\n",
    "\n",
    "\n",
    "\n",
    "[\n",
    "    \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
    "    \n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n",
    "\n",
    "]\n",
    "\n",
    "In LangChain there is a slightly different format. We use three message objects like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because res is just another AIMessage object, we can append it to messages, add another HumanMessage, and generate the next response in the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"I would like to understand string theory\")\n",
    "\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Hallucinations\n",
    "\n",
    "The knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the parametric knowledge of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "The result of this is very clear when we ask LLMs about more recent information, like about the new (and very popular) Llama 2 LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"What is so special about Llama 2?\"\n",
    ")\n",
    "\n",
    "# add to messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "]\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the model answer is totally wrong. Llama 2 is not a game. Lets try model response for LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Can you tell me about the LLMChain in LangChain?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear from this answer that the LLM doesn't know the informaiton, but sometimes an LLM may respond like it does know the answer — and this can be very hard to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding source knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way of feeding knowledge into LLMs. It is called source knowledge and it refers to any information fed into the LLM via the prompt. We can try that with the LLMChain question. We can take a description of this object from the LangChain documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now feed this information to our chat bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of this answer is phenomenal. This is made possible due to the augmention of our query with external knowledge (source knowledge). We can use the concept of vector databases to get this information automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Data\n",
    "\n",
    "In this task, we will be importing our data. We will be using the Hugging Face Datasets library to load our data. Specifically, we will be using the <b>\"jamescalam/llama-2-arxiv-papers\"</b> dataset. This dataset contains a collection of ArXiv papers which will serve as the external knowledge base for our chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\n",
    "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "    split=\"train\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Overview\n",
    "\n",
    "The dataset we are using is sourced from the Llama 2 ArXiv papers. It is a collection of academic papers from ArXiv, a repository of electronic preprints approved for publication after moderation. Each entry in the dataset represents a \"chunk\" of text from these papers.\n",
    "\n",
    "Because most Large Language Models (LLMs) only contain knowledge of the world as it was during training, they cannot answer our questions about Llama 2 — at least not without this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Knowledge Base\n",
    "\n",
    "We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using HuggingFace model to generate embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf_embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_id,\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'my name is xyz'\n",
    "]\n",
    "\n",
    "res = hf_embedding_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we get two (aligning to our two chunks of text) 384-dimensional embeddings.\n",
    "\n",
    "We're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "# Define a vector data base client\n",
    "chroma_client = chromadb.Client()\n",
    "#chroma_client.delete_collection(name=\"my_collections2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from chromadb.db.base import UniqueConstraintError\n",
    "from chromadb.utils import embedding_functions\n",
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"db/\")  # data stored in 'db' folder\n",
    "em = embedding_functions.SentenceTransformerEmbeddingFunction(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(\"lang_chain_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.create_collection(name=\"lang_chain_1\",embedding_function=em)\n",
    "\n",
    "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
    "batch_size = 100\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    doc_ids = [f\"{x['doi']}\" for i, x in batch.iterrows()]\n",
    "    # get text to embed\n",
    "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = hf_embedding.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['chunk'],\n",
    "         'source': x['source'],\n",
    "         'title': x['title']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Adding collections\n",
    "    collection.add(\n",
    "                documents= texts,\n",
    "                metadatas=metadata,\n",
    "                embeddings=embeds,\n",
    "                ids=ids\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the most relevant result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the top 2 results\n",
    "results = collection.query(\n",
    "    query_texts='What is so special about Llama 2',\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "print(results['documents'][0][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation\n",
    "We've built a fully-fledged knowledge base. Now it's time to connect that knowledge base to our chatbot. To do that we'll be diving back into LangChain and reusing our template prompt from earlier.\n",
    "\n",
    "To use LangChain here we need to load the LangChain abstraction for a vector index, called a vectorstore. We pass in our vector index to initialize the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to connect the output from our vectorstore to our chat chatbot. To do that we can use the same logic as we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 5 results from knowledge base\n",
    "    results = collection.query(\n",
    "            query_texts=query,\n",
    "            n_results=5\n",
    "        )\n",
    "\n",
    "    source_knowledge= \"\"\n",
    "    # get the text from the results\n",
    "    for i in range(0, len(results['documents'])):\n",
    "        source_knowledge+= \"\\n\".join(results['documents'][i])\n",
    "    \n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    \n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue with more Llama 2 questions. Let's try without RAG first:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"what safety measures were used in the development of llama 2?\"\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chatbot is able to respond about Llama 2 thanks to it's conversational history stored in messages. However, it doesn't know anything about the safety measures themselves as we have not provided it with that information via the RAG pipeline. Let's try again but with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"what safety measures were used in the development of llama 2?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
