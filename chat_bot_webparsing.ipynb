{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) Implementation With LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red '> <b> What is RAG ? </b> </span>\n",
    "\n",
    "RAG is a technique for augmenting LLM (Large Language Models) knowledge with additional data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics but their knowledge is to the public data up to a specific point in time that they were trained on </span>. \n",
    "\n",
    "If we want to build AI applications that can reason about private data or data introduced after a models cutoff date, we need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as <b> Retrieval Augmented Generation (RAG) </b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red '> <b> What is LangChain ? </b> </span>\n",
    "\n",
    "LangChain is an open source framework that lets software developers working with artificial intelligence (AI) and its machine learning subset combine large language models with other external components to develop LLM-powered applications. LangChain makes it easy to link powerful LLMs, such as OpenAI's GPT-3.5 and GPT-4, to an array of external data sources to create and reap the benefits of natural language processing (NLP) applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple Chat Bot using LangChain and OpenAI\n",
    "\n",
    "We will rely on the LangChain library to bring together the different components needed for the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step-1</b>\n",
    "\n",
    "We run the following command to set up the <i>OpenAI key</i> as enviornment variable (re-execute the set up if kernel res-starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE : You need an API Key from OpenAI to use this functionality\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Setting up the openAI key as environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the Chat GPT 3.5 object to generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Get the OpenAI key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    "# Create the OpenAI object \n",
    "# The temperature value ranges from 0 to 2, with lower values indicating \n",
    "# greater determinism, and higher values indicating more randomness.\n",
    "llm_chat = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step -2 </b>\n",
    "\n",
    "Now we have to build a query message and send it to OpenAI service. But before that we need to understand how to structure our query.\n",
    "\n",
    "<i> Chats with *OpenAI's gpt-3.5-turbo and gpt-4 chat models* are typically structured (in plain text) like this:</i>\n",
    "\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "User: Hi AI, how are you today?\n",
    "\n",
    "Assistant: I'm great thank you. How can I help you?\n",
    "\n",
    "User: I'd like to understand string theory.\n",
    "\n",
    "In the official OpenAI ChatCompletion endpoint these would be passed to the model in a format like:\n",
    "\n",
    "[\n",
    "    \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
    "    \n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LangChain uses a slightly different format. We build the above message format as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "# Prepare LangChain message format\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    HumanMessage(content=\"I will like to know about Australia day in few sentences.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step -3 </b>\n",
    "\n",
    "Send the formatted message to ChatGPT to get a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia Day is a national holiday in Australia celebrated on January 26th each year. It commemorates the arrival of the First Fleet of British ships in 1788, which marked the beginning of European settlement in Australia. The day is often marked with various events and activities, including fireworks, concerts, parades, and citizenship ceremonies. However, it is also a day of reflection and debate, as it is seen by some as a celebration of colonization and the dispossession of Indigenous Australians.\n"
     ]
    }
   ],
   "source": [
    "# Invoke OpenAI servcie to get a response\n",
    "response = llm_chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because response is just another AIMessage object, we can append it to messages, add another HumanMessage, and generate the next response in the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Australia Day is a public holiday in Australia. It is a day off for the general population, and most businesses and schools are closed. It is a time for people to come together and celebrate the nation's history, culture, and achievements.\n"
     ]
    }
   ],
   "source": [
    "# Add latest AI response to messages\n",
    "messages.append(response)\n",
    "\n",
    "# Now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Is Australian day a public holiday ?\")\n",
    "\n",
    "# Add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# Send query to OpenAI service\n",
    "response = llm_chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional Work # 1** \n",
    "We can change the persona of ChatGPT. Such as provide output in French or any language other than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour! Je vais bien, merci de demander. Et vous? Comment puis-je vous aider aujourd'hui?\n"
     ]
    }
   ],
   "source": [
    "# Preparing LangChain message prompt\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates from english to french.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),    \n",
    "    HumanMessage(content=\"How was Neploean ?\")\n",
    "]\n",
    "\n",
    "response = llm_chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional Work # 2** \n",
    "We can change the persona of ChatGPT. Such as enforcing OpenAI service to produce output in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"response\": \"I'm sorry, but I cannot provide information about Napoleon as I am an AI assistant and do not have access to real-time data. However, Napoleon Bonaparte was a prominent military and political leader in the late 18th and early 19th centuries. He was the Emperor of the French and is known for his military campaigns and reforms in France.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Preparing LangChain message prompt\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that parses json output.\"),\n",
    "    HumanMessage(content=\"How was Neploean ? Produce answer in json format\")\n",
    "]\n",
    "\n",
    "# Create the OpenAI object \n",
    "# The temperature value ranges from 0 to 2, with lower values indicating \n",
    "# greater determinism, and higher values indicating more randomness.\n",
    "llm_chat_json = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "response = llm_chat_json.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step -4 Dealing with Hallucinations </b>\n",
    "\n",
    "The knowledge of LLMs can be limited because LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. This knowledge is called the parametric knowledge of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "So, we expect to get hallucinated output from LLM if we ask about a more recent information. Such as enquiring about LLAMA 2 language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have any information about a specific llama named \"llama 2.\" Llamas are domesticated South American camelids known for their woolly coats and gentle temperament. They are often used as pack animals and are also kept as pets or for their fiber. Llamas are herbivores and primarily eat grass, hay, and other plants. They are social animals and live in herds, communicating with each other through various vocalizations and body language. Llamas have been domesticated for thousands of years and are valued for their wool, meat, and as guard animals for livestock.\n"
     ]
    }
   ],
   "source": [
    "# Preparing LangChain message prompt\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Can you please tell me about llama 2 ?\")\n",
    "]\n",
    "\n",
    "# Getting response from OpenAI\n",
    "response = llm_chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that OpenAI model failed to provide the output. To tackle this issue, we feed knowledge into LLMs in another way. It is called <b>source knowledge and it refers to any information fed into the LLM via the prompt</b>. We can do that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating source knowledge\n",
    "llama2_information = [\n",
    "    \"Code Llama is a code generation model built on Llama 2, trained on 500B tokens of code. It supports common programming languages being used today, including Python, C++, Java, PHP, Typescript (Javascript), C#, and Bash.\",\n",
    "    \"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.\"\n",
    "]\n",
    "source_knowledge = \"\\n\".join(llama2_information)\n",
    "\n",
    "# Creating a query with source knowledge\n",
    "query = \"Can you tell me about the llama 2 ?\"\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "        Contexts:\n",
    "        {source_knowledge}\n",
    "\n",
    "        Query: {query}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed this additional information along with query to the model. \n",
    "\n",
    "Now, the quality of this answer is phenomenal. This is made possible due to the augmention of our query with external knowledge (source knowledge). We can use the concept of vector databases to get this information automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) developed and released by the authors. These models range in scale from 7 billion to 70 billion parameters. One specific variant of Llama 2 is called Llama 2-Chat, which is optimized for dialogue use cases. The authors claim that their Llama 2-Chat models outperform open-source chat models on most benchmarks tested. They have also conducted human evaluations for helpfulness and safety, suggesting that Llama 2-Chat may be a suitable substitute for closed-source models. The authors provide a detailed description of their approach to fine-tuning and safety improvements of Llama 2-Chat, with the intention of enabling the community to build on their work and contribute to the responsible development of LLMs.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = llm_chat.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step -5 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 5.1 - Load data from webpages </b>\n",
    "\n",
    "We will perform web scrapping to read data from <i>multiple urls or webpages</i> to help our chatbot answer questions about the latest and greatest in the world of GenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323\n",
      "9979\n",
      "16271\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Create a loader object\n",
    "html_loader = WebBaseLoader([\"https://ai.meta.com/resources/models-and-libraries/llama/\",\\\n",
    "                        \"https://zapier.com/blog/llama-meta/\",\\\n",
    "                        \"https://en.wikipedia.org/wiki/LLaMA\"])\n",
    "\n",
    "# Load data from html pages\n",
    "data = html_loader.load()\n",
    "print(len(data[0].page_content))\n",
    "print(len(data[1].page_content))\n",
    "print(len(data[2].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step 5.2 - Indexing: Split</b>\n",
    "\n",
    "Ther loaded document is over 25k characters long. This is too long to fit in the context window of many models. To handle this we'll split the `Document` into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time.\n",
    "\n",
    "We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. We use the [RecursiveCharacterTextSplitter](/docs/modules/data_connection/document_transformers/recursive_text_splitter), which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
    "\n",
    "We set `add_start_index=True` so that the character index at which each split Document starts within the initial Document is preserved as metadata attribute \"start_index\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Update Your Browser | Facebook\\n\\n\\n\\nUpdate Your BrowserYou’re using a web browser that isn’t supported by Facebook.To get a better experience, go to one of these sites and get the latest version of your preferred browser:Google ChromeMozilla FirefoxGet Facebook on Your PhoneStay connected anytime, anywhere.', metadata={'source': 'https://ai.meta.com/resources/models-and-libraries/llama/', 'title': 'Update Your Browser | Facebook', 'language': 'en', 'start_index': 2}), Document(page_content=\"What is Llama 2 and why does it matter?Skip to contentProductZapier Automation PlatformNo-code automation across 6,000+ appsHow it worksLearn the basicsSecurityTrusted by 2M+ businessesFeaturesBuild flexible workflowsApp integrationsExplore 6,000+ app connectionsWhat's newTablesNo-code databases built for ZapsInterfacesBetaCustom pages to power your ZapsChatbotsBetaEasy to build, no code requiredAI featuresBetaAccess our latest AI-powered featuresExplore app integrationsJoin Zapier Early AccessSolutionsBy use caseLead managementSales pipelineMarketing campaignsCustomer supportData managementProject managementTickets and incidentsBy appSalesforceMicrosoft Dynamics CRMHubSpotMarketoSlackMicrosoft TeamsZendeskJira Software CloudJira Service ManagementBy teamMarketingLeadersITSales operationsBy company sizeStartupsSmall and medium businessesEnterpriseHow Zapier ZapsHow Zapier’s RevOps team automates lead managementHow Zapier Uses AIUsing AI & Zapier in Marketing, Sales, & RevOpsExplore app\", metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'What is Llama 2 and why does it matter?', 'description': \"Llama 2 is Meta's open source large language model (LLM). It's freely available for almost anyone to use for research and commercial purposes—here's why that matters.\", 'language': 'en', 'start_index': 0}), Document(page_content=\"& RevOpsExplore app integrationsJoin Zapier Early AccessResources & SupportBy teamMarketingLeadersITSales operationsLearn moreBlogZapier LearnEvents and webinarsCustomer storiesZapier guidesGet helpHelp CenterCommunityHire an ExpertSupport ServicesContact SupportZapier quick-start guideCreate your first Zap with easeDeveloper resourcesDeveloper PlatformBuild an integrationEmbed an integrationIntegration Partner ProgramDocumentationExplore app integrationsJoin Zapier Early AccessPricingLoadingLoadingHomeProductivityApp tipsApp tips5 min readWhat is Llama 2 and why does it matter?By Harry Guinness · August 16, 2023Llama 2 is Meta's open source large language model (LLM). It's basically the Facebook parent company's response to OpenAI's GPT models and Google's AI models like PaLM 2—but with one key difference: it's freely available for almost anyone to use for research and commercial purposes.\\xa0That's a pretty big deal, and it could blow the whole AI space wide open. Let me explain.\\xa0What\", metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'What is Llama 2 and why does it matter?', 'description': \"Llama 2 is Meta's open source large language model (LLM). It's freely available for almost anyone to use for research and commercial purposes—here's why that matters.\", 'language': 'en', 'start_index': 981}), Document(page_content='me explain.\\xa0What is Llama 2?Llama 2 is a family of LLMs like GPT-3 and PaLM 2. While there are some technical differences between it and other LLMs, you would really need to be deep into AI for them to mean much. All these LLMs were developed and work in essentially the exact same way; they all use the same transformer architecture and development ideas like pretraining and fine-tuning.Try Zapier ChatbotsCreate free custom AI chatbots to engage customers and take action with built-in automation.Get startedWhen you enter a text prompt or provide Llama 2 with text input in some other way, it attempts to predict the most plausible follow-on text using its neural network—a cascading algorithm with billions of variables (called \"parameters\") that\\'s modeled after the human brain. By assigning different weights to all the different parameters, and throwing in a small bit of randomness, Llama 2 can generate incredibly human-like responses.\\xa0How to try Llama 2Llama 2 doesn\\'t yet have a flashy,', metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'What is Llama 2 and why does it matter?', 'description': \"Llama 2 is Meta's open source large language model (LLM). It's freely available for almost anyone to use for research and commercial purposes—here's why that matters.\", 'language': 'en', 'start_index': 1963}), Document(page_content='yet have a flashy, easy-to-use demo application like ChatGPT or Google Bard. For now, the best way to try it out is through Hugging Face, the platform that\\'s become the go-to hub for open source AI models. Through Hugging Face, you can try out the following versions of Llama 2:Llama 2 7B ChatLlama 2 13B ChatLlama 2 70B ChatThat \"Chat\" at the end indicates that they\\'re using a fine-tuned version of each model called Llama-2-chat, which is optimized for chatbot-like dialogue—similar to how ChatGPT is a fine-tuned, chatbot-optimized version of GPT.You\\'ll also notice three different sizes: 7B with seven billion parameters, 13B with 13 billion parameters, and 70B with 70 billion parameters. While all are optimized for speed, the smaller sizes will run significantly quicker on lower spec hardware, even if they aren\\'t quite as effective at generating plausible or accurate text.Bear in mind that right out of the box, Llama 2 simply isn\\'t as good as ChatGPT for many tasks, especially if you\\'re', metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'What is Llama 2 and why does it matter?', 'description': \"Llama 2 is Meta's open source large language model (LLM). It's freely available for almost anyone to use for research and commercial purposes—here's why that matters.\", 'language': 'en', 'start_index': 2943}), Document(page_content='if you\\'re using GPT-4. Both the foundation models and the fine-tuned chat models are designed to be further trained to meet your specific needs (more on that in a second).\\xa0How does Llama 2 work?To create its neural network, Llama 2 was trained with 2 trillion \"tokens\" from publicly available sources like Common Crawl (an archive of billions of webpages), Wikipedia, and public domain books from Project Gutenberg. Each token is a word or semantic fragment that allows the model to assign meaning to text and plausibly predict follow-on text. If the words \"Apple\" and \"iPhone\" consistently appear together, it\\'s able to understand that the two concepts are related—and are distinct from \"apple,\" \"banana,\" and \"fruit.\"Of course, training an AI model on the open internet is a recipe for racism and other horrendous content, so the developers also employed other training strategies, including reinforcement learning with human feedback (RLHF), to optimize the model for safe and helpful responses.', metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'What is Llama 2 and why does it matter?', 'description': \"Llama 2 is Meta's open source large language model (LLM). It's freely available for almost anyone to use for research and commercial purposes—here's why that matters.\", 'language': 'en', 'start_index': 3933}), Document(page_content=\"helpful responses. With RLHF, human testers rank different responses from the AI model to steer it toward generating more appropriate outputs. The chat versions were also fine-tuned with specific data to make them better at responding to dialogue in a natural way.But even these models are just intended to be a base to build from. If you want to create an LLM to generate article summaries in your company's particular brand style or voice, you can train Llama 2 with dozens, hundreds, or even thousands of examples and create one that does just that. Similarly, you can further fine-tune one of the chat-optimized models to respond to your customer support requests by providing it with your FAQs and other relevant information like chat logs.\\xa0Llama vs. GPT, Bard, and other AI models: How do they compare?In the research paper describing how they developed Llama 2, the researchers compare its performance on various benchmarks (like the multi-task language understanding and TriviaQA reading\", metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'What is Llama 2 and why does it matter?', 'description': \"Llama 2 is Meta's open source large language model (LLM). It's freely available for almost anyone to use for research and commercial purposes—here's why that matters.\", 'language': 'en', 'start_index': 4913}), Document(page_content='TriviaQA reading comprehension dataset) to other open source and closed source models, including the big names like GPT-3.5, GPT-4, PaLM, and PaLM 2. In short, the 70B versions of Llama outperform the other open source LLMs and are generally as good as GPT-3.5 and PaLM on most benchmarks, but don\\'t perform as well as GPT-4 or PaLM 2.And that kind of tracks with my testing. I found Llama 2 was more likely to \"hallucinate\" or just make things up when given simple prompts. Though in support of what Meta set out to do, I couldn\\'t trick it into saying anything egregious.\\xa0Joanna Stern has never worked for Wired, and calling her a YouTube personality is a big stretch—she hosts some Wall Street Journal videos on the platform. Wired has a YouTube series called Technique Critique, but it doesn\\'t have a host. Everything else is also a bit suspect.\\xa0\\xa0Similarly, it was suitably silly when asked to write some poetry.Though ChatGPT was a bit more creative.Of course, this isn\\'t a fair test for Llama', metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'What is Llama 2 and why does it matter?', 'description': \"Llama 2 is Meta's open source large language model (LLM). It's freely available for almost anyone to use for research and commercial purposes—here's why that matters.\", 'language': 'en', 'start_index': 5892}), Document(page_content=\"fair test for Llama 2. It's not really trying to be a direct ChatGPT competitor—it's something a little different.\\xa0Why Llama mattersMost of the LLMs you've heard of—OpenAI's GPT-3 and GPT 4, Google's PaLM and PaLM 2, Anthropic's Claude—are all closed source. Researchers and businesses can use the official APIs to access them and even fine-tune versions of their models so they give tailored responses, but they can't really get their hands dirty or understand what's going on inside.With Llama 2, though, you can read the research paper detailing exactly how the model was created and trained. You can also download the model right now, and as long as you have the technical chops, get it running on your computer or even dig into its code. (Though be warned: the smallest version is more than 13 GB.)And much more usefully, you can also get it running on Microsoft Azure, Amazon Web Services, and other cloud infrastructures through platforms like Hugging Face, where you can train it on your own\", metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'What is Llama 2 and why does it matter?', 'description': \"Llama 2 is Meta's open source large language model (LLM). It's freely available for almost anyone to use for research and commercial purposes—here's why that matters.\", 'language': 'en', 'start_index': 6870}), Document(page_content=\"it on your own data to generate the kind of text you need. Just be sure to check out Meta's guide to responsibly using Llama.By being so open with Llama, Meta is making it significantly easier for other companies to develop AI-powered applications that they have more control over. The only real limits to the license are that companies with more than 700 million monthly users have to ask for special permission to use Llama, so it's off limits for the likes of Apple, Google, and Amazon.And really, that's quite exciting. So many of the big developments in computing over the past 70 years have been built on top of open research and experimentation, and now AI looks set to be one of them. While Google and OpenAI are always going to be players in the space, they won't be able to build the kind of commercial moat or consumer lock-in that Google has in search and advertising.\\xa0By letting Llama out into the world, there will likely always be a credible alternative to closed source AIs.Related\", metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'What is Llama 2 and why does it matter?', 'description': \"Llama 2 is Meta's open source large language model (LLM). It's freely available for almost anyone to use for research and commercial purposes—here's why that matters.\", 'language': 'en', 'start_index': 7855}), Document(page_content=\"source AIs.Related reading:ChatGPT vs. Bard: What's the difference?ChatGPT vs. Bing Chat: Which AI chatbot should you use?The best AI productivity toolsWhat is Google Gemini?Get productivity tips delivered straight to your inboxSubscribeWe’ll email you 1-3 times per week—and never share your information.Harry GuinnessHarry Guinness is a writer and photographer from Dublin, Ireland. His writing has appeared in the New York Times, Lifehacker, the Irish Examiner, and How-To Geek. His photos have been published on hundreds of sites—mostly without his permission.tagsArtificial intelligence (AI)Related articlesApp tipsHow to record a Zoom meetingHow to record a Zoom meetingApp tipsHow to set Slack remindersHow to set Slack remindersApp tipsHow to send a test email in MailchimpHow to send a test email in MailchimpApp tipsHow to create and manage a Slack channelHow to create and manage a Slack channelImprove your productivity automatically. Use Zapier to get your apps working together.Sign\", metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'What is Llama 2 and why does it matter?', 'description': \"Llama 2 is Meta's open source large language model (LLM). It's freely available for almost anyone to use for research and commercial purposes—here's why that matters.\", 'language': 'en', 'start_index': 8834}), Document(page_content='together.Sign upSee how Zapier worksPricingHelpDeveloper PlatformPressJobsZapier for CompaniesTemplatesFollow usZapier© 2024 Zapier Inc.Manage cookiesLegalPrivacy', metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'What is Llama 2 and why does it matter?', 'description': \"Llama 2 is Meta's open source large language model (LLM). It's freely available for almost anyone to use for research and commercial purposes—here's why that matters.\", 'language': 'en', 'start_index': 9817}), Document(page_content='LLaMA - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload file\\n\\n\\n\\n\\n\\nLanguages\\n\\nLanguage links are at the top of the page.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\n Create account Log in\\n\\n\\n\\n\\n\\n\\t\\tPages for logged out editors learn more\\n\\n\\n\\nContributionsTalk\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n(Top)\\n\\n\\n\\n\\n\\n1LLaMA-2\\n\\n\\n\\n\\n\\n\\n\\n2Architecture and training\\n\\n\\n\\nToggle Architecture and training subsection\\n\\n\\n\\n\\n\\n2.1Architecture\\n\\n\\n\\n\\n\\n\\n\\n2.2Training datasets\\n\\n\\n\\n\\n\\n\\n\\n2.3Fine-tuning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3Release and leak\\n\\n\\n\\n\\n\\n\\n\\n4Dataset reproduction\\n\\n\\n\\n\\n\\n\\n\\n5Applications\\n\\n\\n\\n\\n\\n\\n\\n6References\\n\\n\\n\\n\\n\\n\\n\\n7Further reading\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nLLaMA\\n\\n\\n\\n13 languages', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 4}), Document(page_content='13 languages\\n\\n\\n\\n\\nCatalàEspañolفارسیFrançais한국어עברית日本語PortuguêsRuna SimiРусскийSuomiУкраїнська中文\\n\\nEdit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLWikidata item\\n\\n\\n\\n\\n\\n\\t\\tPrint/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 985}), Document(page_content=\"Large language model by Meta AI\\nFor the animal, see Llama.\\nNot to be confused with LaMDA.\\nFor other uses, see Llama (disambiguation).\\nLLaMA (Large Language Model Meta AI) is a family of autoregressive large language models (LLMs), released by Meta AI starting in February 2023. \\nFor the first version of LLaMA, four model sizes were trained: 7, 13, 33, and 65 billion parameters. LLaMA's developers reported that the 13B parameter model's performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters) and that the largest model was competitive with state of the art models such as PaLM and Chinchilla.[1] Whereas the most powerful LLMs have generally been accessible only through limited APIs (if at all), Meta released LLaMA's model weights to the research community under a noncommercial license.[2] Within a week of LLaMA's release, its weights were leaked to the public on 4chan via BitTorrent.[3]\", metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 1521}), Document(page_content='In July 2023, Meta released several models as Llama 2, using 7, 13 and 70 billion parameters.', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 2458}), Document(page_content='LLaMA-2[edit]\\nOn July 18, 2023, in partnership with Microsoft, Meta announced LLaMA-2, the next generation of LLaMA. Meta trained and released LLaMA-2 in three model sizes: 7, 13, and 70 billion parameters.[4] The model architecture remains largely unchanged from that of LLaMA-1 models, but 40% more data was used to train the foundational models.[5] The accompanying preprint[5] also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.\\nLLaMA-2 includes both foundational models and models fine-tuned for dialog, called LLaMA-2 Chat. In further departure from LLaMA-1, all models are released with weights, and are free for many commercial use cases. However, due to some remaining restrictions, the description of LLaMA as open source has been disputed by the Open Source Initiative (known for maintaining the Open Source Definition).[6]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 2554}), Document(page_content=\"In November 2023, research conducted by Patronus AI, an artificial intelligence startup company, compared performance of LLaMA-2, OpenAI's GPT-4 and GPT-4-Turbo, and Anthropic's Claude2 on two versions of a 150-question test about information in SEC filings (e.g. Form 10-K, Form 10-Q, Form 8-K, earnings reports, earnings call transcripts) submitted by public companies to the agency where one version of the test required the generative AI models to use a retrieval system to locate the specific SEC filing to answer the questions while the other version provided the specific SEC filing to the models to answer the question (i.e. in a long context window). On the retrieval system version, GPT-4-Turbo and LLaMA-2 both failed to produce correct answers to 81% of the questions, while on the long context window version, GPT-4-Turbo and Claude-2 failed to produce correct answers to 21% and 24% of the questions respectively.[7][8]\", metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 3448}), Document(page_content='Architecture and training[edit]\\n\\n\"A conversation between the sun and pluto\"\\n\\nSun: Hey Pluto, what’s up?\\nPluto: Not much. Just hanging out here at the edge of the solar system.\\nSun: Yeah, I’ve been meaning to talk to you about that. You’re not a planet anymore.\\nPluto: What? Come on, man. That’s not fair.\\nSun: I’m sorry, but it’s true. You just don’t meet the criteria anymore.\\nPluto: This is bulls**t! I’ve been a planet for over 70 years!\\nSun: Things change, Pluto. You’ve just got to accept it.\\nPluto: F**k you, Sun! F**k you and your stupid solar system!\\n\\n\\n– Output of 65 billion parameter LLaMA model after instruction tuning given the prompt \"Write a conversation between the sun and pluto\"[1]\\n\\n\\nArchitecture[edit]\\nLLaMA uses the transformer architecture, the standard architecture for language modeling since 2018. \\nThere are minor architectural differences. Compared to GPT-3, LLaMA', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 4383}), Document(page_content=\"uses SwiGLU[9] activation function instead of ReLU;\\nuses rotary positional embeddings[10] instead of absolute positional embedding;\\nuses root-mean-squared layer-normalization[11] instead of standard layer-normalization.[12]\\nincreases context length from 2K (Llama 1) tokens to 4K (Llama 2) tokens between.\\nTraining datasets[edit]\\nLLaMA's developers focused their effort on scaling the model's performance by increasing the volume of training data, rather than the number of parameters, reasoning that the dominating cost for LLMs is from doing inference on the trained model rather than the computational cost of the training process. \\nLLaMA 1 foundational models were trained on a data set with 1.4 trillion tokens, drawn from publicly available data sources, including:[1]\", metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 5276}), Document(page_content='Webpages scraped by CommonCrawl\\nOpen source repositories of source code from GitHub\\nWikipedia in 20 different languages\\nPublic domain books from Project Gutenberg\\nThe LaTeX source code for scientific papers uploaded to ArXiv\\nQuestions and answers from Stack Exchange websites', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 6052}), Document(page_content='Llama 2 foundational models were trained on a data set with 2 trillion tokens. This data set was curated to remove Web sites that often disclose personal data of people. It also upsamples sources considered trustworthy.[5] Llama 2 - Chat was additionally fine-tuned on 27,540 prompt-response pairs created for this project, which performed better than larger but lower-quality third-party datasets. For AI alignment, reinforcement learning with human feedback (RLHF) was used with a combination of 1,418,091 Meta examples and seven smaller datasets. The average dialog depth was 3.9 in the Meta examples, 3.0 for Anthropic Helpful and Anthropic Harmless sets, and 1.0 for five other sets, including OpenAI Summarize, StackExchange, etc.', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 6328}), Document(page_content='Fine-tuning[edit]\\nLlama 1 models are only available as foundational models with self-supervised learning and without fine-tuning. Llama 2 – Chat models were derived from foundational Llama 2 models. Unlike GPT-4 which increased context length during fine-tuning, Llama 2 and Llama 2 - Chat have the same context length of 4K tokens. Supervised fine-tuning used an autoregressive loss function with token loss on user prompts zeroed out. Batch size was 64.\\nFor AI alignment, human annotators wrote prompts and then compared two model outputs (a binary protocol), giving confidence levels and separate safety labels with veto power. Two separate reward models were trained from these preferences for safety and helpfulness using Reinforcement learning from human feedback (RLHF). A major technical contribution is the departure from the exclusive use of Proximal Policy Optimization (PPO) for RLHF – a new technique based on Rejection sampling was used, followed by PPO.', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 7066}), Document(page_content='Multi-turn consistency in dialogs was targeted for improvement, to make sure that \"system messages\" (initial instructions, such as \"speak in French\" and \"act like Napoleon\") are respected during the dialog. This was accomplished using the new \"Ghost attention\" technique during training, that concatenates relevant instructions to each new user message but zeros out the loss function for tokens in the prompt (earlier parts of the dialog).', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 8035}), Document(page_content='Release and leak[edit]\\nLLaMA was announced on February 23, 2023, via a blog post and a paper describing the model\\'s training, architecture, and performance.[1][2] The inference code used to run the model was publicly released under the open-source GPL 3 license.[13] Access to the model\\'s weights was managed by an application process, with access to be granted \"on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world\".[2]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 8477}), Document(page_content='On March 2, 2023, a torrent containing LLaMA\\'s weights was uploaded, with a link to the torrent shared on the 4chan imageboard and subsequently spreading through online AI communities.[3] That same day, a pull request on the main LLaMA repository was opened, requesting to add the magnet link to the official documentation.[14][15] On March 4, a pull request was opened to add links to HuggingFace repositories containing the model.[16][14] On March 6, Meta filed takedown requests to remove the HuggingFace repositories linked in the pull request, characterizing it as \"unauthorized distribution\" of the model. HuggingFace complied with the requests.[17] On March 20, Meta filed a DMCA takedown request for copyright infringement against a repository containing a script that downloaded LLaMA from a mirror, and GitHub complied the next day.[18] As of March 25, Facebook has not responded to the pull request containing the magnet link.[15]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 9026}), Document(page_content=\"Reactions to the leak varied. Some speculated that the model would be used for malicious purposes, such as more sophisticated spam. Some have celebrated the model's accessibility, as well as the fact that smaller versions of the model can be run relatively cheaply, suggesting that this will promote the flourishing of additional research developments.[3] Multiple commentators, such as Simon Willison, compared LLaMA to Stable Diffusion, a text-to-image model which, unlike comparably sophisticated models which preceded it, was openly distributed, leading to a rapid proliferation of associated tools, techniques, and software.[3][19]\", metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 9968}), Document(page_content='Dataset reproduction[edit]\\nOn April 17, 2023, TogetherAI launched a project named RedPajama to reproduce and distribute an open source version of the LLaMA dataset.[20] The dataset has approximately 1.2 trillion tokens and is publicly available for download.[21]\\n\\nApplications[edit]\\nSoftware developer Georgi Gerganov released llama.cpp, a software-optimized re-implementation of LLaMa in C++. This allowed many to  run the LLaMa series of models locally.[22]\\nThe Stanford University Institute for Human-Centered Artificial Intelligence (HAI) Center for Research on Foundation Models (CRFM) released Alpaca, a training recipe based on the LLaMA 7B model that uses the \"Self-Instruct\" method of instruction tuning to acquire capabilities comparable to the OpenAI GPT-3 series text-davinci-003 model at a modest cost.[23][24] Multiple open source projects are[when?] continuing this work of finetuning LLaMA with Alpaca dataset.[25]\\n\\nReferences[edit]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 10606}), Document(page_content='References[edit]\\n\\n\\n^ a b c d Touvron, Hugo; Lavril, Thibaut; Izacard, Gautier; Martinet, Xavier; Lachaux, Marie-Anne; Lacroix, Timothée; Rozière, Baptiste; Goyal, Naman; Hambro, Eric; Azhar, Faisal; Rodriguez, Aurelien; Joulin, Armand; Grave, Edouard; Lample, Guillaume (2023). \"LLaMA: Open and Efficient Foundation Language Models\". arXiv:2302.13971 [cs.CL].\\n\\n^ a b c \"Introducing LLaMA: A foundational, 65-billion-parameter large language model\". Meta AI. 24 February 2023.\\n\\n^ a b c d Vincent, James (8 March 2023). \"Meta\\'s powerful AI language model has leaked online — what happens now?\". The Verge.\\n\\n^ \"Meta and Microsoft Introduce the Next Generation of LLaMA\". Meta. 18 July 2023. Retrieved 21 July 2023.\\n\\n^ a b c Touvron, Hugo; Martin, Louis; et\\xa0al. (18 Jul 2023). \"LLaMA-2: Open Foundation and Fine-Tuned Chat Models\". arXiv:2307.09288 [cs.CL].', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 11538}), Document(page_content='^ Edwards, Benj (2023-07-18). \"Meta launches LLaMA-2, a source-available AI model that allows commercial applications [Updated]\". Ars Technica. Retrieved 2023-08-08.\\n\\n^ Leswing, Kif (December 19, 2023). \"GPT and other AI models can\\'t analyze an SEC filing, researchers find\". CNBC. Retrieved December 19, 2023.\\n\\n^ \"Patronus AI Launches Industry-first LLM Benchmark for Finance to Address Hallucinations\" (Press release). PR Newswire. November 16, 2023. Retrieved December 19, 2023.\\n\\n^ Shazeer, Noam (2020-02-01). \"GLU Variants Improve Transformer\". arXiv:2104.09864 [cs.CL].\\n\\n^ Su, Jianlin; Lu, Yu; Pan, Shengfeng; Murtadha, Ahmed; Wen, Bo; Liu, Yunfeng (2021-04-01). \"RoFormer: Enhanced Transformer with Rotary Position Embedding\". arXiv:2104.09864 [cs.CL].\\n\\n^ Zhang, Biao; Sennrich, Rico (2019-10-01). \"Root Mean Square Layer Normalization\". arXiv:1910.07467 [cs.LG].\\n\\n^ Lei Ba, Jimmy; Kiros, Jamie Ryan; Hinton, Geoffrey E. (2016-07-01). \"Layer Normalization\". arXiv:1607.06450 [stat.ML].', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 12393}), Document(page_content='^ \"llama\". GitHub. Retrieved 16 March 2023.\\n\\n^ a b VK, Anirudh (6 March 2023). \"Meta\\'s LLaMA Leaked to the Public, Thanks To 4chan\". Analytics India Magazine. Retrieved 17 March 2023.\\n\\n^ a b \"Save bandwidth by using a torrent to distribute more efficiently by ChristopherKing42 · Pull Request #73 · facebookresearch/llama\". GitHub. Retrieved 25 March 2023.\\n\\n^ \"Download weights from huggingface to help us save bandwith by Jainam213 · Pull Request #109 · facebookresearch/llama\". GitHub. Retrieved 17 March 2023.\\n\\n^ Cox, Joseph (7 March 2023). \"Facebook\\'s Powerful Large Language Model Leaks Online\". Vice. Retrieved 17 March 2023.\\n\\n^ OpSec Online LLC (21 March 2023). \"github/dmca - Notice of Claimed Infringement via Email\". GitHub. Retrieved 25 March 2023.\\n\\n^ Willison, Simon (11 March 2023). \"Large language models are having their Stable Diffusion moment\". Simon Willison\\'s Weblog.', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 13386}), Document(page_content='^ \"RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset\". GitHub. Together. Retrieved 4 May 2023.\\n\\n^ \"RedPajama-Data-1T\". Hugging Face. Together. Retrieved 4 May 2023.\\n\\n^ Edwards, Benj (2023-03-13). \"You can now run a GPT-3-level AI model on your laptop, phone, and Raspberry Pi\". Ars Technica. Retrieved 2024-01-04.\\n\\n^ Taori, Rohan; Gulrajani, Ishaan; Zhang, Tianyi; Dubois, Yann; Li, Xuechen; Guestrin, Carlos; Liang, Percy; Hashimoto, Tatsunori B. (13 March 2023). \"Alpaca: A Strong, Replicable Instruction-Following Model\". Stanford Center for Research on Foundation Models.\\n\\n^ Wang, Yizhong; Kordi, Yeganeh; Mishra, Swaroop; Liu, Alisa; Smith, Noah A.; Khashabi, Daniel; Hajishirzi, Hannaneh (2022). \"Self-Instruct: Aligning Language Models with Self-Generated Instructions\". arXiv:2212.10560 [cs.CL].\\n\\n^ \"alpaca-lora\". GitHub. Retrieved 5 April 2023.\\n\\n\\nFurther reading[edit]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 14274}), Document(page_content='Huang, Kalley; O\\'Regan, Sylvia Varnham (September 5, 2023). \"Inside Meta\\'s AI Drama: Internal Feuds Over Compute Power\". The Information. Archived from the original on September 5, 2023. Retrieved September 6, 2023.\\n\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=LLaMA&oldid=1198400974\"\\nCategories: 2023 softwareInternet leaksLarge language modelsMeta PlatformsArtificial intelligenceHidden categories: Articles with short descriptionShort description matches WikidataAll articles with vague or ambiguous timeVague or ambiguous time from August 2023\\n\\n\\n\\n\\n\\n\\n This page was last edited on 24 January 2024, at 01:20\\xa0(UTC).\\nText is available under the Creative Commons Attribution-ShareAlike License 4.0;\\nadditional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 15177}), Document(page_content='Privacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle limited content width', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'LLaMA - Wikipedia', 'language': 'en', 'start_index': 16094})]\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=20, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "print(all_splits)\n",
    "print(len(all_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step 5.3- Store indexes to vector database </b>\n",
    "\n",
    "We need to index our 34 text chunks so that we can search over them at runtime. The most common way to do this is to embed the contents of each document split and insert these embeddings into a vector database (or vector store). \n",
    "\n",
    "We take a text search query, embed it, and perform some sort of \"similarity\" search (cosine similarity) to identify the relevant splits most similar to our query. We will use <i>chroma vector database.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Create an embedding model to generate embeddings from text chunks\n",
    "# Any embedding model can be used\n",
    "model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_id,\n",
    "    model_kwargs=model_kwargs)\n",
    "\n",
    "# Generate and store embeddings to vector store\n",
    "vector_store = Chroma.from_documents(documents=all_splits, embedding=hf_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step 5.3 - Retrieve the relevant content from vector database </b>\n",
    "\n",
    "Create a retriever object and can then pass a query to get the relevanr output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='sophisticated models which preceded it, was openly distributed, leading to a rapid proliferation of associated tools, techniques, and software.[3][19]', metadata={'language': 'en', 'source': 'https://en.wikipedia.org/wiki/LLaMA', 'start_index': 10454, 'title': 'LLaMA - Wikipedia'}), Document(page_content='sophisticated models which preceded it, was openly distributed, leading to a rapid proliferation of associated tools, techniques, and software.[3][19]', metadata={'language': 'en', 'source': 'https://en.wikipedia.org/wiki/LLaMA', 'start_index': 10454, 'title': 'LLaMA - Wikipedia'}), Document(page_content='^ Willison, Simon (11 March 2023). \"Large language models are having their Stable Diffusion moment\". Simon Willison\\'s Weblog.\\n\\n^ \"RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset\". GitHub. Together. Retrieved 4 May 2023.\\n\\n^ \"RedPajama-Data-1T\". Hugging Face. Together. Retrieved 4 May 2023.\\n\\n^ Edwards, Benj (2023-03-13). \"You can now run a GPT-3-level AI model on your laptop, phone, and Raspberry Pi\". Ars Technica. Retrieved 2024-01-04.', metadata={'language': 'en', 'source': 'https://en.wikipedia.org/wiki/LLaMA', 'start_index': 14147, 'title': 'LLaMA - Wikipedia'})]\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever object to get top 3 matching chunks\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "# Use query to get relevant output\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "print(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step 5.4 - Implementing RAG </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function to use query to extract top 3 matching results from vector database. The extracted information is used as source knowledge and passed along with the query to the OpenAI service to provide a meaningful output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 results from knowledge base (vector database) \n",
    "def augment_prompt(query: str, top_n : int):\n",
    "    # Create a retriever object to get top 3 matching chunks\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": top_n})\n",
    "    # Use query to get relevant output\n",
    "    retrieved_docs = retriever.invoke(query)        \n",
    "    # Get the text from the results\n",
    "    source_knowledge= \"\"\n",
    "    for i in range(0, len(retrieved_docs)):\n",
    "        source_knowledge+= \"\\n\".join(retrieved_docs[i].page_content)    \n",
    "    \n",
    "    # Feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "        Contexts:\n",
    "        {source_knowledge}\n",
    "\n",
    "        Query: {query}\"\"\"\n",
    "    \n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The augmented prompt is ready. We pass this prompt to the OpenAI service to get a reliable answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output without RAG : I'm sorry, but I don't have any information about a specific llama named \"llama 2.\" Llamas are domesticated South American camelids known for their woolly coats and gentle nature. They are often used as pack animals and are also kept as pets or for their fiber. Llamas are herbivores and primarily eat grass and hay. They are social animals and live in herds, communicating with each other through various vocalizations and body language. Llamas have been domesticated for thousands of years and are valued for their strength, agility, and ability to adapt to different climates and terrains.\n",
      "Output with RAG : Llama 2 is a model that allows you to read the research paper detailing exactly how the model was created and trained. You can also download the model and run it on your computer or dig into its code. However, please note that the smallest version of the model is more than 13 GB in size.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "# Create the OpenAI object \n",
    "# The temperature value ranges from 0 to 2, with lower values indicating \n",
    "# greater determinism, and higher values indicating more randomness.\n",
    "llm_chat_new = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "# Prepare LangChain message format\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\")\n",
    "]\n",
    "\n",
    "query = \"Can you please tell me about llama 2 ?\"\n",
    "\n",
    "# Without RAG \n",
    "prompt = HumanMessage(\n",
    "    content=query)\n",
    "messages.append(prompt)\n",
    "response = llm_chat_new.invoke(messages)\n",
    "print(f\"Output without RAG : {response.content}\")\n",
    "\n",
    "# With RAG \n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query,2))\n",
    "messages.append(prompt)\n",
    "response = llm_chat_new.invoke(messages)\n",
    "print(f\"Output with RAG : {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similary user can ask about any recent information and will get an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output without RAG : I'm sorry, but I don't have any information about a specific llama 2 or its safety measures. Could you please provide more context or clarify your question?\n",
      "Output with RAG : The provided context does not mention any specific safety measures used in Llama 2. It mainly discusses the release and architecture of the model.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "# Preparing LangChain message prompt\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Can you please tell me about llama 2 ?\")\n",
    "]\n",
    "\n",
    "query= \"What safety measures were used in llama 2?\"\n",
    "\n",
    "# Without RAG \n",
    "prompt = HumanMessage(\n",
    "    content=query)\n",
    "messages.append(prompt)\n",
    "response = llm_chat_new.invoke(messages)\n",
    "print(f\"Output without RAG : {response.content}\")\n",
    "\n",
    "# With RAG and without RAG\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query,3))\n",
    "messages.append(prompt)\n",
    "response = llm_chat_new.invoke(messages)\n",
    "print(f\"Output with RAG : {response.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
