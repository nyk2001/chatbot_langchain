{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) Implementation With LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red '> <b> What is RAG ? </b> </span>\n",
    "\n",
    "RAG is a technique for augmenting LLM (Large Language Models) knowledge with additional data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics but their knowledge is to the public data up to a specific point in time that they were trained on </span>. \n",
    "\n",
    "If we want to build AI applications that can reason about private data or data introduced after a models cutoff date, we need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as <b> Retrieval Augmented Generation (RAG) </b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red '> <b> What is LangChain ? </b> </span>\n",
    "\n",
    "LangChain is an open source framework that lets software developers working with artificial intelligence (AI) and its machine learning subset combine large language models with other external components to develop LLM-powered applications. LangChain makes it easy to link powerful LLMs, such as OpenAI's GPT-3.5 and GPT-4, to an array of external data sources to create and reap the benefits of natural language processing (NLP) applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple Chat Bot using LangChain and OpenAI\n",
    "\n",
    "We will rely on the LangChain library to bring together the different components needed for the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step-1</b>\n",
    "\n",
    "We run the following command to set up the <i>OpenAI key</i> as enviornment variable (re-execute the set up if kernel res-starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE : You need an API Key from OpenAI to use this functionality\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Setting up the openAI key as environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the Chat GPT 3.5 object to generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Get the OpenAI key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    "# Create the OpenAI object \n",
    "# The temperature value ranges from 0 to 2, with lower values indicating \n",
    "# greater determinism, and higher values indicating more randomness.\n",
    "llm_chat = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step -2 </b>\n",
    "\n",
    "Now we have to build a query message and send it to OpenAI service. But before that we need to understand how to structure our query.\n",
    "\n",
    "<i> Chats with *OpenAI's gpt-3.5-turbo and gpt-4 chat models* are typically structured (in plain text) like this:</i>\n",
    "\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "User: Hi AI, how are you today?\n",
    "\n",
    "Assistant: I'm great thank you. How can I help you?\n",
    "\n",
    "User: I'd like to understand string theory.\n",
    "\n",
    "In the official OpenAI ChatCompletion endpoint these would be passed to the model in a format like:\n",
    "\n",
    "[\n",
    "    \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
    "    \n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LangChain uses a slightly different format. We build the above message format as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "# Prepare LangChain message format\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    HumanMessage(content=\"I will like to know about Australia day in few sentences.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step -3 </b>\n",
    "\n",
    "Send the formatted message to ChatGPT to get a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia Day is a national holiday in Australia celebrated on January 26th each year. It marks the anniversary of the arrival of the First Fleet at Port Jackson in New South Wales in 1788, which led to the establishment of the first European settlement in Australia. The day is a time for Australians to come together to celebrate their country, culture, and achievements. However, it is also a day that is controversial for many Indigenous Australians, who see it as a day of mourning and invasion. The date has sparked debate and calls for it to be changed to a more inclusive day that recognizes the history and culture of all Australians.\n"
     ]
    }
   ],
   "source": [
    "# Invoke OpenAI servcie to get a response\n",
    "response = llm_chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because response is just another AIMessage object, we can append it to messages, add another HumanMessage, and generate the next response in the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Australia Day is a public holiday in Australia. It is a day off for the general population, and many businesses and government offices are closed on this day. There are usually various events and activities held across the country to celebrate Australia Day, including fireworks, concerts, barbecues, and citizenship ceremonies.\n"
     ]
    }
   ],
   "source": [
    "# Add latest AI response to messages\n",
    "messages.append(response)\n",
    "\n",
    "# Now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Is Australian day a public holiday ?\")\n",
    "\n",
    "# Add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# Send query to OpenAI service\n",
    "response = llm_chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional Work # 1** \n",
    "We can change the persona of ChatGPT. Such as provide output in French or any language other than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour AI, comment vas-tu aujourd'hui ?\n"
     ]
    }
   ],
   "source": [
    "# Preparing LangChain message prompt\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates from english to french.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),    \n",
    "    HumanMessage(content=\"How was Neploean ?\")\n",
    "]\n",
    "\n",
    "response = llm_chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional Work # 2** \n",
    "We can change the persona of ChatGPT. Such as enforcing OpenAI service to produce output in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"response\": \"I'm sorry, but I cannot provide information on how Napoleon was as it is a historical question. If you have any specific questions or need assistance with something else, feel free to ask!\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Preparing LangChain message prompt\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that parses json output.\"),\n",
    "    HumanMessage(content=\"How was Neploean ? Produce answer in json format\")\n",
    "]\n",
    "\n",
    "# Create the OpenAI object \n",
    "# The temperature value ranges from 0 to 2, with lower values indicating \n",
    "# greater determinism, and higher values indicating more randomness.\n",
    "llm_chat_json = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "response = llm_chat_json.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step -4 Dealing with Hallucinations </b>\n",
    "\n",
    "The knowledge of LLMs can be limited because LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. This knowledge is called the parametric knowledge of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "So, we expect to get hallucinated output from LLM if we ask about a more recent information. Such as enquiring about LLAMA 2 language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have any information about \"llama 2.\" Could you provide more context or clarify your question so I can better assist you?\n"
     ]
    }
   ],
   "source": [
    "# Preparing LangChain message prompt\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Can you please tell me about llama 2 ?\")\n",
    "]\n",
    "\n",
    "# Getting response from OpenAI\n",
    "response = llm_chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that OpenAI model failed to provide the output. To tackle this issue, we feed knowledge into LLMs in another way. It is called <b>source knowledge and it refers to any information fed into the LLM via the prompt</b>. We can do that as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle this issue, we feeding knowledge into LLMs in another way. It is called source knowledge and it refers to any information fed into the LLM via the prompt. We can try that with the Llama 2 question. We can take a description of this object from the Llama 2 source page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating source knowledge\n",
    "llama2_information = [\n",
    "    \"Code Llama is a code generation model built on Llama 2, trained on 500B tokens of code. It supports common programming languages being used today, including Python, C++, Java, PHP, Typescript (Javascript), C#, and Bash.\",\n",
    "    \"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.\"\n",
    "]\n",
    "source_knowledge = \"\\n\".join(llama2_information)\n",
    "\n",
    "# Creating a query with source knowledge\n",
    "query = \"Can you tell me about the llama 2 ?\"\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "        Contexts:\n",
    "        {source_knowledge}\n",
    "\n",
    "        Query: {query}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed this additional information to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) developed for dialogue use cases. Ranging in scale from 7 billion to 70 billion parameters, Llama 2 includes models optimized for chat applications, known as Llama 2-Chat. These models have shown superior performance compared to open-source chat models across various benchmarks. Additionally, human evaluations have indicated that Llama 2-Chat models are helpful and safe, potentially serving as viable alternatives to closed-source models. The creators of Llama 2 have provided detailed insights into their fine-tuning process and safety enhancements, encouraging the community to leverage their work and contribute to the responsible advancement of large language models.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = llm_chat.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of this answer is phenomenal. This is made possible due to the augmention of our query with external knowledge (source knowledge). We can use the concept of vector databases to get this information automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building RAG Chatbots with LangChain\n",
    "\n",
    "In this example, we will build an AI chatbot from start-to-finish so that it can answer automatically about Llama 2 instead of providing the information manually (<b>source knowledge provided using vector database</b>). We will be using LangChain,HuggingFace embeddings, OpenAI, and vector DB, to build a chatbot capable of learning from the external world using Retrieval Augmented Generation (RAG).\n",
    "\n",
    "We will use two techniques to build our chatbot:\n",
    "\n",
    "1- Scrap a dataset sourced from the Llama 2 ArXiv paper and other related papers to help our chatbot answer questions about the latest and greatest in the world of GenAI.\n",
    "\n",
    "2- Scrap multiple webpages to help our chatbot answer questions about the latest and greatest in the world of GenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:red '> <b> Techniue 2 </b> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <span style='color:blue '>1. Load data from webpages </span></b>\n",
    "\n",
    "We will perform web scrapping to read data from <i>multiple urls or webpages</i> to help our chatbot answer questions about the latest and greatest in the world of GenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316\n",
      "11914\n",
      "24415\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Create a loader object\n",
    "html_loader = WebBaseLoader([\"https://ai.meta.com/resources/models-and-libraries/llama/\",\\\n",
    "                        \"https://zapier.com/blog/llama-meta/\",\\\n",
    "                        \"https://en.wikipedia.org/wiki/LLaMA\"])\n",
    "\n",
    "# Load data from html pages\n",
    "data = html_loader.load()\n",
    "print(len(data[0].page_content))\n",
    "print(len(data[1].page_content))\n",
    "print(len(data[2].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <span style='color:blue '>2. Split document into chunks </span></b>\n",
    "\n",
    "Ther loaded document is over 25k characters long. This is too long to fit in the context window of many models. To handle this we'll split the `Document` into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time.\n",
    "\n",
    "We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. We use the [RecursiveCharacterTextSplitter](/docs/modules/data_connection/document_transformers/recursive_text_splitter), which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
    "\n",
    "We set `add_start_index=True` so that the character index at which each split Document starts within the initial Document is preserved as metadata attribute \"start_index\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Update Your Browser\\n\\n\\n\\n\\nUpdate Your BrowserYou’re using a web browser that isn’t supported by Facebook.To get a better experience, go to one of these sites and get the latest version of your preferred browser:Google ChromeMozilla FirefoxGet Facebook on Your PhoneStay connected anytime, anywhere.', metadata={'source': 'https://ai.meta.com/resources/models-and-libraries/llama/', 'title': 'Update Your Browser', 'language': 'en', 'start_index': 2}), Document(page_content=\"Meta AI: What is Llama 3 and why does it matter?Skip to contentProductZapier Automation PlatformNo-code automation across 7,000+ appsPRODUCTSZapsDo-it-yourself automation for workflowsTablesDatabases designed for workflowsInterfacesCustom pages to power your workflowsCAPABILITIESApp integrationsExplore 7,000+ app integrationsAI automationCutting-edge AI to upgrade your workflowsSecurityEnterprise-grade securityWhat's newCanvasBetaPlan and map your workflows with AIAI ChatbotBetaAnswer customer questions with AI chatbotsCentralBetaCreate your own AI bots for any taskExplore templatesExplore use casesJoin Zapier Early AccessSolutionsSolutionsHow Zapier can help you automate your work across teamsBy teamRevOpsDrive revenue through automationMarketingMultiply campaign effectiveness and ROIITBetter manage systems with automationSalesClose more dealsCustomer SupportElevate customer satisfactionLeadersStreamline decision-making processesBy appSalesforceHubSpotSlackOpenAIMicrosoft Dynamics\", metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'Meta AI: What is Llama 3 and why does it matter?', 'description': \"Llama 3 is Meta's open source large language model (LLM). It's freely available for almost anyone to use via Meta AI and for research and commercial purposes.\", 'language': 'en', 'start_index': 0}), Document(page_content=\"Dynamics CRMMicrosoft TeamsZendeskJira Software CloudMarketoView all appsBy use caseLead managementAutomate your way to more conversionsSales pipelineAutomate handoffs and streamline salesMarketing campaignsBoost your marketing campaign's impactCustomer supportQuickly route tickets and responsesData managementConnect app data for more insightsProject managementDeliver projects seamlesslyTickets and incidentsResolve issues and incidents fasterAutomation for RevOpsSee how Zapier can take your RevOps and GTM engine to the next levelZapier forStartupsSmall and medium businessesEnterpriseExplore templatesExplore use casesJoin Zapier Early AccessResourcesBy teamMarketingLeadersITSales operationsLearn moreBlogZapier LearnEvents and webinarsCustomer storiesZapier guidesGet helpHelp CenterCommunityHire an ExpertSupport ServicesMore about Zapier SupportZapier quick-start guideCreate your first Zap with easeDeveloper resourcesDeveloper PlatformBuild an integrationEmbed an integrationIntegration\", metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'Meta AI: What is Llama 3 and why does it matter?', 'description': \"Llama 3 is Meta's open source large language model (LLM). It's freely available for almost anyone to use via Meta AI and for research and commercial purposes.\", 'language': 'en', 'start_index': 988}), Document(page_content=\"Partner ProgramDocumentationExplore templatesExplore use casesJoin Zapier Early AccessEnterprisePricingLoadingLoadingHomeProductivityApp tipsApp tips6 min readMeta AI: What is Llama 3 and why does it matter?By Harry Guinness · April 19, 2024Llama 3 is Meta's latest family of open source large language models (LLM). It's basically the Facebook parent company's response to OpenAI's GPT and Google's Gemini—but with one key difference: it's freely available for almost anyone to use for research and commercial purposes.\\xa0That's a pretty big deal, and over the past year, Llama 2, the previous model family, has become a staple of open source AI developments. Llama 3 continues that promise. Let me explain.\\xa0What is Llama 3?Llama 3 is a family of LLMs like GPT-4 and Google Gemini. It's the successor to Llama 2, Meta's previous generation of AI models. While there are some technical differences between Llama and other LLMs, you would really need to be deep into AI for them to mean much. All these\", metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'Meta AI: What is Llama 3 and why does it matter?', 'description': \"Llama 3 is Meta's open source large language model (LLM). It's freely available for almost anyone to use via Meta AI and for research and commercial purposes.\", 'language': 'en', 'start_index': 1987}), Document(page_content='much. All these LLMs were developed and work in essentially the exact same way; they all use the same transformer architecture and development ideas like pretraining and fine-tuning.Try Zapier ChatbotsCreate free custom AI chatbots to engage customers and take action with built-in automation.Get startedWhen you enter a text prompt or provide Llama 3 with text input in some other way, it attempts to predict the most plausible follow-on text using its neural network—a cascading algorithm with billions of variables (called \"parameters\") that\\'s modeled after the human brain. By assigning different weights to all the different parameters, and throwing in a small bit of randomness, Llama 3 can generate incredibly human-like responses.\\xa0Meta has released four versions of Llama 3 so far:Llama 3 8BLlama 3 8B-InstructLlama 3 70BLlama 3 70B-InstructThe 8B models have 8 billion parameters, while the two 70B models have 70 billion parameters. Both instruct models were fine-tuned to better follow', metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'Meta AI: What is Llama 3 and why does it matter?', 'description': \"Llama 3 is Meta's open source large language model (LLM). It's freely available for almost anyone to use via Meta AI and for research and commercial purposes.\", 'language': 'en', 'start_index': 2971}), Document(page_content=\"to better follow human directions, so they're more suited to use as a chatbot than the raw Llama models.\\xa0Meta is training a 400 billion parameter version of Llama 3 (and presumably a 400 billion parameter instruct version, too) that it hopes to make available later this year. Given the size and complexity of the model, though, it just isn't ready yet.Like the latest models from OpenAI and Google, Meta is also developing a multimodal version of Llama 3. This will allow it to work with other modalities, like images, handwritten text, video footage, and audio clips. It's not available yet but should be released in the coming months. Similarly, Meta is training multilingual versions of Llama 3, but they aren't available yet. Meta AI: How to try Llama 3Meta AI, the AI assistant built into Facebook, Messenger, Instagram, and WhatsApp, now uses Llama 3. You can also check it out using a newly released dedicated web app, Meta AI.If you aren't in one of the handful of countries where Meta has\", metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'Meta AI: What is Llama 3 and why does it matter?', 'description': \"Llama 3 is Meta's open source large language model (LLM). It's freely available for almost anyone to use via Meta AI and for research and commercial purposes.\", 'language': 'en', 'start_index': 3951}), Document(page_content='where Meta has launched Meta AI, you can demo the 70B-Instruct model using HuggingChat, AI repository HuggingSpace\\'s example chatbot. How does Llama 3 work?To create its neural network, Llama 3 was trained with over 15 trillion \"tokens\"—the overall dataset was seven times larger than that used to train Llama 2. Some of the data comes from publicly available sources like Common Crawl (an archive of billions of webpages), Wikipedia, and public domain books from Project Gutenberg, while some of it was also reportedly generated by AI. (None of it is Meta user data.) Each token is a word or semantic fragment that allows the model to assign meaning to text and plausibly predict follow-on text. If the words \"Apple\" and \"iPhone\" consistently appear together, it\\'s able to understand that the two concepts are related—and are distinct from \"apple,\" \"banana,\" and \"fruit.\" According to Meta, Llama 3\\'s tokenizer has a larger vocabulary than Llama 2\\'s, so it\\'s significantly more efficient.Of course,', metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'Meta AI: What is Llama 3 and why does it matter?', 'description': \"Llama 3 is Meta's open source large language model (LLM). It's freely available for almost anyone to use via Meta AI and for research and commercial purposes.\", 'language': 'en', 'start_index': 4935}), Document(page_content=\"course, training an AI model on the open internet is a recipe for racism and other horrendous content, so the developers also employed other training strategies, including reinforcement learning with human feedback (RLHF), to optimize the model for safe and helpful responses. With RLHF, human testers rank different responses from the AI model to steer it toward generating more appropriate outputs. The instruct versions were also fine-tuned with specific data to make them better at responding to human instructions in a natural way.Meta has also developed Llama Guard and Llamma Code Shield, two safety models designed to prevent Llama 3 from running harmful prompts or generating insecure computer code.But all these Llama models are just intended to be a base for developers to build from. If you want to create an LLM to generate article summaries in your company's particular brand style or voice, you can train Llama 3 with dozens, hundreds, or even thousands of examples and create one\", metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'Meta AI: What is Llama 3 and why does it matter?', 'description': \"Llama 3 is Meta's open source large language model (LLM). It's freely available for almost anyone to use via Meta AI and for research and commercial purposes.\", 'language': 'en', 'start_index': 5927}), Document(page_content=\"and create one that does just that. Similarly, you can further fine-tune one of the instruct models to respond to your customer support requests by providing it with your FAQs and other relevant information like chat logs.\\xa0Or you can just take Llama 3 and retrain it to create your own completely independent LLM.Llama vs. GPT, Gemini, and other AI models: How do they compare?In the blog post announcing Llama 3 (the research paper is still forthcoming), Meta's\\xa0researchers compare the 8B and 70B Instruct models'\\xa0performance on various benchmarks (like the multi-task language understanding and ARC-challenge common sense logic test) to a handful of equivalent open source and closed source models. The 8B model is compared to Mistral 7B and Gemma 7B, while the 70B model is compared to Gemini Pro 1.0 and Mixtral 8x22B. In what can only be called cherry-picked examples, the Llama 3 models are all the top performers.In a head-to-head human evaluation challenge, Llama 3 70B-Instruct apparently\", metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'Meta AI: What is Llama 3 and why does it matter?', 'description': \"Llama 3 is Meta's open source large language model (LLM). It's freely available for almost anyone to use via Meta AI and for research and commercial purposes.\", 'language': 'en', 'start_index': 6908}), Document(page_content='apparently compares favorably to Claude Sonnet, GPT-3.5, and Mistral Medium.While Meta doesn\\'t compare Llama to the current state-of-the-art models, like GPT-4, Claude Opus, and Gemini 2, the company presumably is waiting until its 400B model is ready for prime time.\\xa0In my testing, I found Llama 3 was a big step up from Llama 2. I couldn\\'t get it to \"hallucinate\" or just make things up anywhere near as easily. While it isn\\'t yet replacing ChatGPT, Meta probably isn\\'t wrong to call it \"the most capable openly available LLM to date.\"Why Llama mattersMost of the LLMs you\\'ve heard of—OpenAI\\'s GPT-3 and GPT 4, Google\\'s Gemini, Anthropic\\'s Claude—are all proprietary and closed source. Researchers and businesses can use the official APIs to access them and even fine-tune versions of their models so they give tailored responses, but they can\\'t really get their hands dirty or understand what\\'s going on inside.With Llama 3, though, you can download the model right now, and as long as you have', metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'Meta AI: What is Llama 3 and why does it matter?', 'description': \"Llama 3 is Meta's open source large language model (LLM). It's freely available for almost anyone to use via Meta AI and for research and commercial purposes.\", 'language': 'en', 'start_index': 7895}), Document(page_content=\"as long as you have the technical chops, get it running on your computer or even dig into its code. (Though be warned: even small LLMs are measured in GBs.) Meta also plans to publish a full research paper detailing how all three models were trained once the 400B model is ready, and there's plenty of interesting information about what the Meta AI team is doing in the blog post...if you're the kind of person who finds these things interesting.Teach AI bots to work across your favorite appsTry Zapier CentralAnd much more usefully, you can also get it running on Microsoft Azure, Amazon Web Services, and other cloud infrastructures through platforms like Hugging Face, where you can train it on your own data to generate the kind of text you need. Just be sure to check out Meta's guide to responsibly using Llama.By continuing to be so open with Llama, Meta is making it significantly easier for other companies to develop AI-powered applications that they have more control over—as long as\", metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'Meta AI: What is Llama 3 and why does it matter?', 'description': \"Llama 3 is Meta's open source large language model (LLM). It's freely available for almost anyone to use via Meta AI and for research and commercial purposes.\", 'language': 'en', 'start_index': 8873}), Document(page_content=\"over—as long as they stick to the acceptable use policy. The only big limits to the license are that companies with more than 700 million monthly users have to ask for special permission to use Llama, so the likes of Apple, Google, and Amazon have to develop their own LLMs.And really, that's quite exciting. So many of the big developments in computing over the past 70 years have been built on top of open research and experimentation, and now AI looks set to be one of them. While Google, OpenAI, and Anthropic are always going to be players in the space, they won't be able to build the kind of commercial moat or consumer lock-in that Google has in search and advertising.\\xa0By letting Llama out into the world, there will likely always be a credible alternative to closed source AIs.Related reading:The best AI productivity toolsWhat is Google Gemini?What is Sora? OpenAI's text-to-video modelWhat is Perplexity AI?Meta AI vs. ChatGPT: Which is better?This article was originally published in\", metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'Meta AI: What is Llama 3 and why does it matter?', 'description': \"Llama 3 is Meta's open source large language model (LLM). It's freely available for almost anyone to use via Meta AI and for research and commercial purposes.\", 'language': 'en', 'start_index': 9853}), Document(page_content='published in August 2023. The most recent update was in April 2024.Get productivity tips delivered straight to your inboxSubscribeWe’ll email you 1-3 times per week—and never share your information.Harry GuinnessHarry Guinness is a writer and photographer from Dublin, Ireland. His writing has appeared in the New York Times, Lifehacker, the Irish Examiner, and How-To Geek. His photos have been published on hundreds of sites—mostly without his permission.tagsArtificial intelligence (AI)Related articlesApp tips70+ AI art styles to use in your AI prompts70+ AI art styles to use in your AI promptsApp tipsHow to use Feathery for better no-code forms and workflowsHow to use Feathery for better no-code forms...App tipsStay on track with this Notion goals templateStay on track with this Notion goals...App tipsHow to use ChatGPTHow to use ChatGPTImprove your productivity automatically. Use Zapier to get your apps working together.Sign upSee how Zapier worksPricingHelpDeveloper', metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'Meta AI: What is Llama 3 and why does it matter?', 'description': \"Llama 3 is Meta's open source large language model (LLM). It's freely available for almost anyone to use via Meta AI and for research and commercial purposes.\", 'language': 'en', 'start_index': 10837}), Document(page_content='PlatformPressJobsEnterpriseTemplatesFollow usZapier© 2024 Zapier Inc.Manage cookiesLegalPrivacy', metadata={'source': 'https://zapier.com/blog/llama-meta/', 'title': 'Meta AI: What is Llama 3 and why does it matter?', 'description': \"Llama 3 is Meta's open source large language model (LLM). It's freely available for almost anyone to use via Meta AI and for research and commercial purposes.\", 'language': 'en', 'start_index': 11819}), Document(page_content='Llama (language model) - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload file\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\n Create account Log in\\n\\n\\n\\n\\n\\n\\t\\tPages for logged out editors learn more\\n\\n\\n\\nContributionsTalk\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n(Top)\\n\\n\\n\\n\\n\\n1\\nBackground\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\nInitial release\\n\\n\\n\\n\\nToggle Initial release subsection\\n\\n\\n\\n\\n\\n2.1\\nLeak\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3\\nLlama 2\\n\\n\\n\\n\\n\\n\\n\\n\\n4\\nLlama 3\\n\\n\\n\\n\\n\\n\\n\\n\\n5\\nComparison of models\\n\\n\\n\\n\\n\\n\\n\\n\\n6\\nArchitecture and training\\n\\n\\n\\n\\nToggle Architecture and training subsection\\n\\n\\n\\n\\n\\n6.1\\nArchitecture\\n\\n\\n\\n\\n\\n\\n\\n\\n6.2\\nTraining datasets\\n\\n\\n\\n\\n\\n\\n\\n\\n6.3\\nFine-tuning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n7\\nApplications\\n\\n\\n\\n\\nToggle Applications subsection\\n\\n\\n\\n\\n\\n7.1\\nllama.cpp', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 4}), Document(page_content='7.1\\nllama.cpp\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n8\\nReception\\n\\n\\n\\n\\n\\n\\n\\n\\n9\\nSee also\\n\\n\\n\\n\\n\\n\\n\\n\\n10\\nReferences\\n\\n\\n\\n\\n\\n\\n\\n\\n11\\nFurther reading\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nLlama (language model)\\n\\n\\n\\n15 languages\\n\\n\\n\\n\\nCatalàDeutschEspañolفارسیFrançais한국어עברית日本語PortuguêsRuna SimiРусскийСаха тылаSuomiУкраїнська中文\\n\\nEdit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLDownload QR codeWikidata item\\n\\n\\n\\n\\n\\n\\t\\tPrint/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 985}), Document(page_content='(Redirected from LLaMA)\\nLarge language model by Meta AI\\nFor the animal, see Llama. For other uses, see Llama (disambiguation).\\nNot to be confused with LaMDA.\\nLlamaDeveloper(s)Meta AIInitial releaseFebruary\\xa024, 2023; 15 months ago\\xa0(2023-02-24)Stable releaseLlama 3\\n   / April\\xa018, 2024; 44 days ago\\xa0(2024-04-18)\\nRepositorygithub.com/meta-llama/llama3Written inPythonType\\nLarge language model\\nGPT\\nFoundation model\\nLicenseMeta Llama 3 Community License[1]Websitellama.meta.com\\nLlama (acronym for Large Language Model Meta AI, and formerly stylized as LLaMA) is a family of autoregressive large language models released by Meta AI starting in February 2023.[2][3] The latest version is Llama 3 released in April 2024.[4]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 1736}), Document(page_content='Model weights for the first version of Llama were made available to the research community under a non-commercial license, and access was granted on a case-by-case basis.[5][3] Unauthorized copies of the model were shared via BitTorrent, in response, Meta AI issued DMCA takedown requests against repositories sharing the link on GitHub.[6][7] Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use.[8][9] Llama models are trained at different parameter sizes, typically ranging between 7B and 70B.[4] Originally, Llama was only available as a foundation model.[10] Starting with Llama 2, Meta AI started releasing instruction fine-tuned versions alongside foundation models.[9]\\nAlongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in select regions, and a standalone website. Both services use a Llama 3 model.[11]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 2452}), Document(page_content=\"Background[edit]\\nAfter the release of large language model's such as GPT-3, a focus of research was up-scaling models which in some instances showed major increases in emergent capabilities.[12] The release of ChatGPT, and it's surprise success caused an increase in attention to large language models.[13]\\nCompared with other responses to ChatGPT, Meta's Chief AI scientist Yann LeCun stated that large language models are best for aiding with writing.[14][15][16]\", metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 3389}), Document(page_content='Initial release[edit]\\nLLaMA was announced on February 24, 2023, via a blog post and a paper describing the model\\'s training, architecture, and performance.[2][3] The inference code used to run the model was publicly released under the open-source GPLv3 license.[17] Access to the model\\'s weights was managed by an application process, with access to be granted \"on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world\".[3]\\nLlama was trained on only publicly available information, and was trained at various different model sizes, with the intention to make it more accessible to different hardware.\\nMeta AI reported the 13B parameter model performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters), and the largest 65B model was competitive with state of the art models such as PaLM and Chinchilla.[2]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 3856}), Document(page_content='Leak[edit]\\nOn March 3, 2023, a torrent containing LLaMA\\'s weights was uploaded, with a link to the torrent shared on the 4chan imageboard and subsequently spread through online AI communities.[6] That same day, a pull request on the main LLaMA repository was opened, requesting to add the magnet link to the official documentation.[18][19] On March 4, a pull request was opened to add links to HuggingFace repositories containing the model.[20][18] On March 6, Meta filed takedown requests to remove the HuggingFace repositories linked in the pull request, characterizing it as \"unauthorized distribution\" of the model. HuggingFace complied with the requests.[21] On March 20, Meta filed a DMCA takedown request for copyright infringement against a repository containing a script that downloaded LLaMA from a mirror, and GitHub complied the next day.[7]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 4824}), Document(page_content=\"Reactions to the leak varied. Some speculated that the model would be used for malicious purposes, such as more sophisticated spam. Some have celebrated the model's accessibility, as well as the fact that smaller versions of the model can be run relatively cheaply, suggesting that this will promote the flourishing of additional research developments.[6] Multiple commentators, such as Simon Willison, compared LLaMA to Stable Diffusion, a text-to-image model which, unlike comparably sophisticated models which preceded it, was openly distributed, leading to a rapid proliferation of associated tools, techniques, and software.[6][22]\", metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 5678}), Document(page_content=\"Llama 2[edit]\\nOn July 18, 2023, in partnership with Microsoft, Meta announced Llama 2, the next generation of Llama. Meta trained and released Llama 2 in three model sizes: 7, 13, and 70 billion parameters.[9] The model architecture remains largely unchanged from that of LLaMA-1 models, but 40% more data was used to train the foundational models.[23] The accompanying preprint[23] also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.\\nLlama 2 includes foundation models and models fine-tuned for chat. In a further departure from LLaMA, all models are released with weights and are free for many commercial use cases. However, due to some remaining restrictions, Meta's description of LLaMA as open source has been disputed by the Open Source Initiative (known for maintaining the Open Source Definition).[24]\", metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 6316}), Document(page_content='Code Llama is a fine-tune of Llama 2 with code specific datasets. 7B, 13B, and 34B versions were released on August 24, 2023, with the 70B releasing on the January 29, 2024.[25] Starting with the foundation models from Llama 2, Meta AI would train an additional 500B tokens of code datasets, before an additional 20B token of long-context data, creating the Code Llama foundation models. This foundation model was further trained on 5B instruction following token to create the instruct fine-tune. Another foundation model was created for Python code, which trained on 100B tokens of Python-only code, before the long-context data.[26]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 7185}), Document(page_content='Llama 3[edit]\\nOn April 18, 2024, Meta released Llama-3 with two sizes: 8B and 70B parameters. The models have been pre-trained on approximately 15 trillion tokens of text gathered from “publicly available sources” with the instruct models fine-tuned on “publicly available instruction datasets, as well as over 10M human-annotated examples\". Meta plans on releasing multimodal models, models capable of conversing in multiple languages, and models with larger context windows. A version with 400B+ parameters is currently being trained.[4]\\nMeta AI\\'s testing shows that Llama 3 70B beats Gemini, and Claude in most benchmarks.[27][28]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 7822}), Document(page_content=\"During an interview with Dwarkesh Patel, Mark Zuckerberg said the 8B version of Llama 3 was nearly as powerful as the largest Llama 2. That Llama 3 had increased priority in coding abilities based on what was learned from CodeLlama. Compared to previous models, Zuckerberg stated the team was surprised that the 70B model was still learning even at the end on training in the 15T tokens. The decision was made to end training to focus GPU power eleswhere, Zuckerberg stated more research needs to be done on AI data scaling.\\nWhen asked if Meta would continue to open-source models, Zuckerberg stated only if it aligned with Meta's strategy. Zuckerberg floated the possibility of smaller versions of the Llama models for application-specific usecases[29]\", metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 8456}), Document(page_content='Comparison of models[edit]\\n\\n\\nName\\nRelease date\\nParameters\\n\\nTraining cost (petaFLOP-day)\\nContext length\\nCorpus size\\nCommercial viability?\\n\\n\\nLLaMA\\n\\nFebruary 24, 2023\\n\\n\\n6.7B\\n13B\\n32.5B\\n65.2B\\n\\n6,300[30]\\n\\n2048\\n\\n1–1.4T\\n\\nNo\\n\\n\\nLlama 2\\n\\nJuly 18, 2023\\n\\n\\n6.7B\\n13B\\n69B\\n\\n21,000[31]\\n\\n4096\\n\\n2T\\n\\nYes\\n\\n\\nCode Llama\\n\\nAugust 24, 2023\\n\\n\\n6.7B\\n13B\\n33.7B\\n69B\\n\\n\\n\\n\\nLlama 3\\n\\nApril 18, 2024\\n\\n\\n8B\\n70.6B\\n400B+ (unreleased)\\n\\n100,000[32][33]\\n\\n8912\\n\\n15T\\n\\nArchitecture and training[edit]\\n\\n\"A conversation between the sun and pluto\"\\n\\nSun: Hey Pluto, what’s up?\\nPluto: Not much. Just hanging out here at the edge of the solar system.\\nSun: Yeah, I’ve been meaning to talk to you about that. You’re not a planet anymore.\\nPluto: What? Come on, man. That’s not fair.\\nSun: I’m sorry, but it’s true. You just don’t meet the criteria anymore.\\nPluto: This is bulls**t! I’ve been a planet for over 70 years!\\nSun: Things change, Pluto. You’ve just got to accept it.\\nPluto: F**k you, Sun! F**k you and your stupid solar system!', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 9211}), Document(page_content='– Output of 65 billion parameter LLaMA model after instruction tuning given the prompt \"Write a conversation between the sun and pluto\"[2]\\n\\nArchitecture[edit]\\nLLaMA uses the transformer architecture, the standard architecture for language modeling since 2018. \\nThere are minor architectural differences. Compared to GPT-3, LLaMA', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 10193}), Document(page_content=\"uses SwiGLU[34] activation function instead of GeLU;\\nuses rotary positional embeddings[35] instead of absolute positional embedding;\\nuses root-mean-squared layer-normalization[36] instead of standard layer-normalization.[37]\\nIncreases context length from 2K (Llama 1) tokens to 4K (Llama 2) tokens between.\\nTraining datasets[edit]\\nLLaMA's developers focused their effort on scaling the model's performance by increasing the volume of training data, rather than the number of parameters, reasoning that the dominating cost for LLMs is from doing inference on the trained model rather than the computational cost of the training process. \\nLLaMA 1 foundational models were trained on a data set with 1.4 trillion tokens, drawn from publicly available data sources, including:[2]\", metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 10524}), Document(page_content='Webpages scraped by CommonCrawl\\nOpen source repositories of source code from GitHub\\nWikipedia in 20 different languages\\nPublic domain books from Project Gutenberg\\nBooks3 books dataset\\nThe LaTeX source code for scientific papers uploaded to ArXiv\\nQuestions and answers from Stack Exchange websites\\nOn April 17, 2023, TogetherAI launched a project named RedPajama to reproduce and distribute an open source version of the LLaMA dataset.[38] The dataset has approximately 1.2 trillion tokens and is publicly available for download.[39]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 11301}), Document(page_content='Llama 2 foundational models were trained on a data set with 2 trillion tokens. This data set was curated to remove Web sites that often disclose personal data of people. It also upsamples sources considered trustworthy.[23] Llama 2 - Chat was additionally fine-tuned on 27,540 prompt-response pairs created for this project, which performed better than larger but lower-quality third-party datasets. For AI alignment, reinforcement learning with human feedback (RLHF) was used with a combination of 1,418,091 Meta examples and seven smaller datasets. The average dialog depth was 3.9 in the Meta examples, 3.0 for Anthropic Helpful and Anthropic Harmless sets, and 1.0 for five other sets, including OpenAI Summarize, StackExchange, etc.\\nLlama 3 consists of mainly English data, with over 5% in over 30 other languages. Its dataset was filtered by a text-quality classifier, and the classifier was trained by text synthesized by Llama 2.[4]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 11834}), Document(page_content='Fine-tuning[edit]\\nLlama 1 models are only available as foundational models with self-supervised learning and without fine-tuning. Llama 2 – Chat models were derived from foundational Llama 2 models. Unlike GPT-4 which increased context length during fine-tuning, Llama 2 and Llama 2 - Chat have the same context length of 4K tokens. Supervised fine-tuning used an autoregressive loss function with token loss on user prompts zeroed out. The batch size was 64.\\nFor AI alignment, human annotators wrote prompts and then compared two model outputs (a binary protocol), giving confidence levels and separate safety labels with veto power. Two separate reward models were trained from these preferences for safety and helpfulness using Reinforcement learning from human feedback (RLHF). A major technical contribution is the departure from the exclusive use of Proximal Policy Optimization (PPO) for RLHF – a new technique based on Rejection sampling was used, followed by PPO.', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 12776}), Document(page_content='Multi-turn consistency in dialogs was targeted for improvement, to make sure that \"system messages\" (initial instructions, such as \"speak in French\" and \"act like Napoleon\") are respected during the dialog. This was accomplished using the new \"Ghost attention\" technique during training, which concatenates relevant instructions to each new user message but zeros out the loss function for tokens in the prompt (earlier parts of the dialog).', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 13749}), Document(page_content='Applications[edit]\\nThe Stanford University Institute for Human-Centered Artificial Intelligence (HAI) Center for Research on Foundation Models (CRFM) released Alpaca, a training recipe based on the LLaMA 7B model that uses the \"Self-Instruct\" method of instruction tuning to acquire capabilities comparable to the OpenAI GPT-3 series text-davinci-003 model at a modest cost.[40][41][42] The model files were officially removed on March 21st 2023 over hosting costs and safety concerns, though the code and paper remain online for reference.[43][44][45]\\nMeditron is a family of Llama-based finetuned on a corpus of clinical guidelines, PubMed papers, and articles. It was created by researchers at École Polytechnique Fédérale de Lausanne School of Computer and Communication Sciences, and the Yale School of Medicine. It shows increased performance on medical-related benchmarks such as MedQA and MedMCQA.[46][47][48]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 14192}), Document(page_content='Zoom used Meta Llama 2 to create an AI Companion that can summarize meetings, provide helpful presentation tips, and assist with message responses. This AI Companion is powered by multiple models, including Meta Llama 2.[49]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 15110}), Document(page_content=\"llama.cpp[edit]\\nMain article: llama.cpp\\nSoftware developer Georgi Gerganov released llama.cpp as open-source on March 10, 2023. It's a re-implementation of LLaMA in C++, allowing systems without a powerful GPU to run the model locally.[50] The llama.cpp project introduced the GGUF file format, a binary format that stores both tensors and metadata.[51] The format focuses on supporting different quantization types, which can reduce memory usage, and increase speed at the expense of lower model precision.[52]\\nllamafile created by Justine Tunney is an open-source tool that bundles llama.cpp with the model into a single executable file. Tunney et. al. introduced new optimized matrix multiplication kernels for x86 and ARM CPUs, improving prompt evaluation performance for FP16 and 8-bit quantized data types.[53]\", metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 15336}), Document(page_content='Reception[edit]\\nWired describes the 8B parameter version of Llama 3 as being \"surprisingly capable\" given its size.[54]\\nThe response to Meta\\'s integration of Llama into Facebook was mixed, with some users confused after Meta AI told a parental group that it had a child.[55]\\nAccording to the Q4 2023 Earnings transcript, Meta adopted the strategy of open weights to improve on model safety, iteration speed, increase adoption among developers and researchers, and to become the industry standard. Llama 5, 6, and 7 are planned for the future.[56]\\n\\nSee also[edit]\\nMistral AI\\nGPT-4o\\nReferences[edit]\\n\\n\\n^ \"llama3/LICENSE at main · meta-llama/llama3\". GitHub.', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 16154}), Document(page_content='^ a b c d e Touvron, Hugo; Lavril, Thibaut; Izacard, Gautier; Martinet, Xavier; Lachaux, Marie-Anne; Lacroix, Timothée; Rozière, Baptiste; Goyal, Naman; Hambro, Eric; Azhar, Faisal; Rodriguez, Aurelien; Joulin, Armand; Grave, Edouard; Lample, Guillaume (2023). \"LLaMA: Open and Efficient Foundation Language Models\". arXiv:2302.13971 [cs.CL].\\n\\n^ a b c d \"Introducing LLaMA: A foundational, 65-billion-parameter large language model\". Meta AI. 24 February 2023.\\n\\n^ a b c d \"Introducing Meta Llama 3: The most capable openly available LLM to date\". ai.meta.com. April 18, 2024. Retrieved 2024-04-21.\\n\\n^ Malik, Yuvraj; Paul, Katie (25 February 2023). \"Meta heats up Big Tech\\'s AI arms race with new language model\". Reuters.\\n\\n^ a b c d Vincent, James (8 March 2023). \"Meta\\'s powerful AI language model has leaked online — what happens now?\". The Verge.\\n\\n^ a b OpSec Online LLC (21 March 2023). \"github/dmca - Notice of Claimed Infringement via Email\". GitHub. Retrieved 25 March 2023.', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 16811}), Document(page_content='^ David, Emilia (30 October 2023). \"Meta\\'s AI research head wants open source licensing to change\". The Verge.\\n\\n^ a b c \"Meta and Microsoft Introduce the Next Generation of LLaMA\". Meta. 18 July 2023. Retrieved 21 July 2023.\\n\\n^ Peters, Jay; Vincent, James (24 February 2023). \"Meta has a new machine learning language model to remind you it does AI too\". The Verge.\\n\\n^ \"Meet Your New Assistant: Meta AI, Built With Llama 3\". Meta. 18 April 2024.\\n\\n^ \"Examining Emergent Abilities in Large Language Models\". hai.stanford.edu. 13 September 2022.\\n\\n^ \"The inside story of how ChatGPT was built from the people who made it\". MIT Technology Review.\\n\\n^ \"ChatGPT is \\'not particularly innovative,\\' and \\'nothing revolutionary\\', says Meta\\'s chief AI scientist\". ZDNET.\\n\\n^ Badminton, Nik (13 February 2023). \"Meta\\'s Yann LeCun on auto-regressive Large Language Models (LLMs)\". Futurist.com.\\n\\n^ \"Yann LeCun on LinkedIn: My unwavering opinion on current (auto-regressive) LLMs\". www.linkedin.com.', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 17794}), Document(page_content='^ \"llama\". GitHub. Retrieved 16 March 2023.\\n\\n^ a b VK, Anirudh (6 March 2023). \"Meta\\'s LLaMA Leaked to the Public, Thanks To 4chan\". Analytics India Magazine. Retrieved 17 March 2023.\\n\\n^ \"Save bandwidth by using a torrent to distribute more efficiently by ChristopherKing42 · Pull Request #73 · facebookresearch/llama\". GitHub. Retrieved 25 March 2023.\\n\\n^ \"Download weights from hugging face to help us save bandwidth by Jainam213 · Pull Request #109 · facebookresearch/llama\". GitHub. Retrieved 17 March 2023.\\n\\n^ Cox, Joseph (7 March 2023). \"Facebook\\'s Powerful Large Language Model Leaks Online\". Vice. Retrieved 17 March 2023.\\n\\n^ Willison, Simon (11 March 2023). \"Large language models are having their Stable Diffusion moment\". Simon Willison\\'s Weblog.\\n\\n^ a b c Touvron, Hugo; Martin, Louis; et\\xa0al. (18 Jul 2023). \"LLaMA-2: Open Foundation and Fine-Tuned Chat Models\". arXiv:2307.09288 [cs.CL].', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 18777}), Document(page_content='^ Edwards, Benj (2023-07-18). \"Meta launches LLaMA-2, a source-available AI model that allows commercial applications [Updated]\". Ars Technica. Retrieved 2023-08-08.\\n\\n^ \"Introducing Code Llama, a state-of-the-art large language model for coding\". ai.meta.com.\\n\\n^ Rozière, Baptiste; Gehring, Jonas; Gloeckle, Fabian; Sootla, Sten; Gat, Itai; Tan, Xiaoqing Ellen; Adi, Yossi; Liu, Jingyu; Sauvestre, Romain (2024-01-31). \"Code Llama: Open Foundation Models for Code\". arXiv:2308.12950 [cs.CL].\\n\\n^ Wiggers, Kyle (18 April 2024). \"Meta releases Llama 3, claims it\\'s among the best open models available\". TechCrunch.\\n\\n^ Mann, Tobias. \"Meta debuts third-generation Llama large language model\". www.theregister.com.\\n\\n^ Patel, Dwarkesh (15 May 2024). \"Mark Zuckerberg - Llama 3, Open Sourcing $10b Models, & Caesar Augustus\". www.dwarkeshpatel.com.\\n\\n^ \"The Falcon has landed in the Hugging Face ecosystem\". huggingface.co. Retrieved 2023-06-20.', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 19677}), Document(page_content='^ \"llama/MODEL_CARD.md at main · meta-llama/llama\". GitHub. Retrieved 2024-05-28.\\n\\n^ Andrej Karpathy (Apr 18, 2024), The model card has some more interesting info too\\n\\n^ \"llama3/MODEL_CARD.md at main · meta-llama/llama3\". GitHub. Retrieved 2024-05-28.\\n\\n^ Shazeer, Noam (2020-02-01). \"GLU Variants Improve Transformer\". arXiv:2104.09864 [cs.CL].\\n\\n^ Su, Jianlin; Lu, Yu; Pan, Shengfeng; Murtadha, Ahmed; Wen, Bo; Liu, Yunfeng (2021-04-01). \"RoFormer: Enhanced Transformer with Rotary Position Embedding\". arXiv:2104.09864 [cs.CL].\\n\\n^ Zhang, Biao; Sennrich, Rico (2019-10-01). \"Root Mean Square Layer Normalization\". arXiv:1910.07467 [cs.LG].\\n\\n^ Lei Ba, Jimmy; Kiros, Jamie Ryan; Hinton, Geoffrey E. (2016-07-01). \"Layer Normalization\". arXiv:1607.06450 [stat.ML].\\n\\n^ \"RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset\". GitHub. Together. Retrieved 4 May 2023.\\n\\n^ \"RedPajama-Data-1T\". Hugging Face. Together. Retrieved 4 May 2023.', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 20616}), Document(page_content='^ Taori, Rohan; Gulrajani, Ishaan; Zhang, Tianyi; Dubois, Yann; Li, Xuechen; Guestrin, Carlos; Liang, Percy; Hashimoto, Tatsunori B. (13 March 2023). \"Alpaca: A Strong, Replicable Instruction-Following Model\". Stanford Center for Research on Foundation Models.\\n\\n^ Wang, Yizhong; Kordi, Yeganeh; Mishra, Swaroop; Liu, Alisa; Smith, Noah A.; Khashabi, Daniel; Hajishirzi, Hannaneh (2022). \"Self-Instruct: Aligning Language Models with Self-Generated Instructions\". arXiv:2212.10560 [cs.CL].\\n\\n^ \"Stanford CRFM\". crfm.stanford.edu.\\n\\n^ Quach, Katyanna. \"Stanford takes costly, risky Alpaca AI model offline\". www.theregister.com.\\n\\n^ \"Stanford Researchers Take Down Alpaca AI Over Cost and Hallucinations\". Gizmodo. 21 March 2023.\\n\\n^ \"alpaca-lora\". GitHub. Retrieved 5 April 2023.\\n\\n^ \"Meditron: An LLM suite for low-resource medical settings leveraging Meta Llama\". ai.meta.com.\\n\\n^ Petersen, Tanya (28 November 2023). \"EPFL\\'s new Large Language Model for Medical Knowledge\".', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 21569}), Document(page_content='^ \"epfLLM/meditron\". epfLLM. 11 May 2024.\\n\\n^ \"How Companies Are Using Meta Llama\". Meta. 7 May 2024.\\n\\n^ Edwards, Benj (2023-03-13). \"You can now run a GPT-3-level AI model on your laptop, phone, and Raspberry Pi\". Ars Technica. Retrieved 2024-01-04.\\n\\n^ \"GGUF\". huggingface.co. Retrieved 9 May 2024.\\n\\n^ Labonne, Maxime (29 November 2023). \"Quantize Llama models with GGUF and llama.cpp\". Medium. Towards Data Science. Retrieved 9 May 2024.\\n\\n^ Connatser, Matthew. \"Llamafile LLM driver project boosts performance on CPU cores\". www.theregister.com. Retrieved 10 May 2024.\\n\\n^ Knight, Will. \"Meta\\'s Open Source Llama 3 Is Already Nipping at OpenAI\\'s Heels\". Wired.\\n\\n^ \"Meta\\'s amped-up AI agents confusing Facebook users\". ABC News. 19 April 2024.\\n\\n^ https://s21.q4cdn.com/399680738/files/doc_financials/2023/q4/META-Q4-2023-Earnings-Call-Transcript.pdf\\n\\n\\nFurther reading[edit]', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 22539}), Document(page_content='Huang, Kalley; O\\'Regan, Sylvia Varnham (September 5, 2023). \"Inside Meta\\'s AI Drama: Internal Feuds Over Compute Power\". The Information. Archived from the original on September 5, 2023. Retrieved September 6, 2023.\\n\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Llama_(language_model)&oldid=1226672091\"\\nCategories: 2023 softwareInternet leaksLarge language modelsMeta PlatformsHidden categories: Articles with short descriptionShort description matches Wikidata\\n\\n\\n\\n\\n\\n\\n This page was last edited on 1 June 2024, at 04:28\\xa0(UTC).\\nText is available under the Creative Commons Attribution-ShareAlike License 4.0;\\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\n\\n\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle limited content width', metadata={'source': 'https://en.wikipedia.org/wiki/LLaMA', 'title': 'Llama (language model) - Wikipedia', 'language': 'en', 'start_index': 23413})]\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=20, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "print(all_splits)\n",
    "print(len(all_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <span style='color:blue '>3. Store chunks to a vector Database </span></b>\n",
    "\n",
    "We need to index our 34 text chunks so that we can search over them at runtime. The most common way to do this is to embed the contents of each document split and insert these embeddings into a vector database (or vector store). \n",
    "\n",
    "We take a text search query, embed it, and perform some sort of \"similarity\" search (cosine similarity) to identify the relevant splits most similar to our query. We will use <i>chroma vector database.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nabee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Nabee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nabee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Create an embedding model to generate embeddings from text chunks\n",
    "# Any embedding model can be used\n",
    "model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_id,\n",
    "    model_kwargs=model_kwargs)\n",
    "\n",
    "# Generate and store embeddings to vector store\n",
    "vector_store = Chroma.from_documents(documents=all_splits, embedding=hf_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the relevant content from vector database, we create a retriever object and then pass a query to get the relevanr output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"Meta AI: What is Llama 3 and why does it matter?Skip to contentProductZapier Automation PlatformNo-code automation across 7,000+ appsPRODUCTSZapsDo-it-yourself automation for workflowsTablesDatabases designed for workflowsInterfacesCustom pages to power your workflowsCAPABILITIESApp integrationsExplore 7,000+ app integrationsAI automationCutting-edge AI to upgrade your workflowsSecurityEnterprise-grade securityWhat's newCanvasBetaPlan and map your workflows with AIAI ChatbotBetaAnswer customer questions with AI chatbotsCentralBetaCreate your own AI bots for any taskExplore templatesExplore use casesJoin Zapier Early AccessSolutionsSolutionsHow Zapier can help you automate your work across teamsBy teamRevOpsDrive revenue through automationMarketingMultiply campaign effectiveness and ROIITBetter manage systems with automationSalesClose more dealsCustomer SupportElevate customer satisfactionLeadersStreamline decision-making processesBy appSalesforceHubSpotSlackOpenAIMicrosoft Dynamics\", metadata={'description': \"Llama 3 is Meta's open source large language model (LLM). It's freely available for almost anyone to use via Meta AI and for research and commercial purposes.\", 'language': 'en', 'source': 'https://zapier.com/blog/llama-meta/', 'start_index': 0, 'title': 'Meta AI: What is Llama 3 and why does it matter?'}), Document(page_content='^ Taori, Rohan; Gulrajani, Ishaan; Zhang, Tianyi; Dubois, Yann; Li, Xuechen; Guestrin, Carlos; Liang, Percy; Hashimoto, Tatsunori B. (13 March 2023). \"Alpaca: A Strong, Replicable Instruction-Following Model\". Stanford Center for Research on Foundation Models.\\n\\n^ Wang, Yizhong; Kordi, Yeganeh; Mishra, Swaroop; Liu, Alisa; Smith, Noah A.; Khashabi, Daniel; Hajishirzi, Hannaneh (2022). \"Self-Instruct: Aligning Language Models with Self-Generated Instructions\". arXiv:2212.10560 [cs.CL].\\n\\n^ \"Stanford CRFM\". crfm.stanford.edu.\\n\\n^ Quach, Katyanna. \"Stanford takes costly, risky Alpaca AI model offline\". www.theregister.com.\\n\\n^ \"Stanford Researchers Take Down Alpaca AI Over Cost and Hallucinations\". Gizmodo. 21 March 2023.\\n\\n^ \"alpaca-lora\". GitHub. Retrieved 5 April 2023.\\n\\n^ \"Meditron: An LLM suite for low-resource medical settings leveraging Meta Llama\". ai.meta.com.\\n\\n^ Petersen, Tanya (28 November 2023). \"EPFL\\'s new Large Language Model for Medical Knowledge\".', metadata={'language': 'en', 'source': 'https://en.wikipedia.org/wiki/LLaMA', 'start_index': 21569, 'title': 'Llama (language model) - Wikipedia'}), Document(page_content=\"Background[edit]\\nAfter the release of large language model's such as GPT-3, a focus of research was up-scaling models which in some instances showed major increases in emergent capabilities.[12] The release of ChatGPT, and it's surprise success caused an increase in attention to large language models.[13]\\nCompared with other responses to ChatGPT, Meta's Chief AI scientist Yann LeCun stated that large language models are best for aiding with writing.[14][15][16]\", metadata={'language': 'en', 'source': 'https://en.wikipedia.org/wiki/LLaMA', 'start_index': 3389, 'title': 'Llama (language model) - Wikipedia'})]\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever object to get top 3 matching chunks\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "# Use query to get relevant output\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "print(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <span style='color:blue '>4. Implementing RAG </span></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function to use query to extract top 3 matching results from vector database. The extracted information is used as source knowledge and passed along with the query to the OpenAI service to provide a meaningful output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 results from knowledge base (vector database) \n",
    "def augment_prompt(query: str, top_n : int):\n",
    "    # Create a retriever object to get top 3 matching chunks\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": top_n})\n",
    "    # Use query to get relevant output\n",
    "    retrieved_docs = retriever.invoke(query)        \n",
    "    # Get the text from the results\n",
    "    source_knowledge= \"\"\n",
    "    for i in range(0, len(retrieved_docs)):\n",
    "        source_knowledge+= \"\\n\".join(retrieved_docs[i].page_content)    \n",
    "    \n",
    "    # Feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "        Contexts:\n",
    "        {source_knowledge}\n",
    "\n",
    "        Query: {query}\"\"\"\n",
    "    \n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The augmented prompt is ready. We pass this prompt to the OpenAI service to get a reliable answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output without RAG : I'm sorry, but I don't have any information about \"llama 2.\" Could you provide more context or clarify your question so I can better assist you?\n",
      "Output with RAG : Llama 2, announced on July 18, 2023, is the next generation of Llama developed by Meta in partnership with Microsoft. It includes foundational models and fine-tuned models for chat applications. The model architecture remains largely unchanged from the previous LLama-1 models, but 40% more data was used to train the foundational models. Llama 2 was released in three model sizes: 7, 13, and 70 billion parameters. The models are released with weights and are free for many commercial use cases, although there are some remaining restrictions. Model weights for the first version of Llama were made available to the research community under a non-commercial license, and access was granted on a case-by-case basis. Unauthorized copies of the model were shared via BitTorrent, leading to takedown requests from Meta AI against repositories sharing the link on GitHub. Subsequent versions of Llama were made accessible outside academia and released under licenses permitting some commercial use. Llama models are trained at different parameter sizes, typically ranging between 7B and 70B. Initially, Llama was only available as a foundational model. Starting with Llama 2, Meta AI began releasing fine-tuned instruction versions alongside foundational models. Alongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in select regions, using a Llama 3 model for both services.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "# Create the OpenAI object \n",
    "# The temperature value ranges from 0 to 2, with lower values indicating \n",
    "# greater determinism, and higher values indicating more randomness.\n",
    "llm_chat_new = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "# Prepare LangChain message format\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\")\n",
    "]\n",
    "\n",
    "query = \"Can you please tell me about llama 2 ?\"\n",
    "\n",
    "# Without RAG \n",
    "prompt = HumanMessage(\n",
    "    content=query)\n",
    "messages.append(prompt)\n",
    "response = llm_chat_new.invoke(messages)\n",
    "print(f\"Output without RAG : {response.content}\")\n",
    "\n",
    "# With RAG \n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query,2))\n",
    "messages.append(prompt)\n",
    "response = llm_chat_new.invoke(messages)\n",
    "print(f\"Output with RAG : {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similary user can ask about any recent information and will get an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output without RAG : I'm sorry, but I don't have specific information about a llama 2 project or safety measures related to it. If you can provide more context or details, I'd be happy to try to help you further. Alternatively, if you have any general questions about safety measures or best practices, feel free to ask!\n",
      "Output with RAG : Llama 2 incorporated safety measures such as foundational models and fine-tuned models for chat applications. The models were released with weights and were freely available for many commercial use cases. However, there were some restrictions in place, and unauthorized copies of the model were shared via BitTorrent. In response, Meta AI issued DMCA takedown requests against repositories sharing the model links on GitHub. Subsequent versions of Llama were made accessible outside academia and released under licenses allowing for some commercial use.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "# Preparing LangChain message prompt\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Can you please tell me about llama 2 ?\")\n",
    "]\n",
    "\n",
    "query= \"What safety measures were used in llama 2?\"\n",
    "\n",
    "# Without RAG \n",
    "prompt = HumanMessage(\n",
    "    content=query)\n",
    "messages.append(prompt)\n",
    "response = llm_chat_new.invoke(messages)\n",
    "print(f\"Output without RAG : {response.content}\")\n",
    "\n",
    "# With RAG and without RAG\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query,3))\n",
    "messages.append(prompt)\n",
    "response = llm_chat_new.invoke(messages)\n",
    "print(f\"Output with RAG : {response.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
