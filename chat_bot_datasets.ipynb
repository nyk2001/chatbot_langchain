{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) Implementation With LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red '> <b> What is RAG ? </b> </span>\n",
    "\n",
    "RAG is a technique for augmenting LLM (Large Language Models) knowledge with additional data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics but their knowledge is to the public data up to a specific point in time that they were trained on </span>. \n",
    "\n",
    "If we want to build AI applications that can reason about private data or data introduced after a models cutoff date, we need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as <b> Retrieval Augmented Generation (RAG) </b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red '> <b> What is LangChain ? </b> </span>\n",
    "\n",
    "LangChain is an open source framework that lets software developers working with artificial intelligence (AI) and its machine learning subset combine large language models with other external components to develop LLM-powered applications. LangChain makes it easy to link powerful LLMs, such as OpenAI's GPT-3.5 and GPT-4, to an array of external data sources to create and reap the benefits of natural language processing (NLP) applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple Chat Bot using LangChain and OpenAI\n",
    "\n",
    "We will rely on the LangChain library to bring together the different components needed for the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step-1</b>\n",
    "\n",
    "We run the following command to set up the <i>OpenAI key</i> as enviornment variable (re-execute the set up if kernel res-starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the openAI key\n",
    "import getpass\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise Chat GPT 3.5 object to be used for generating responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nabee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# NOTE : You need an API Key from OpenAI to use this functionality\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Creating an OpenAI object\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    "llm_chat = ChatOpenAI(\n",
    "    temperature = 0,\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step -2 </b>\n",
    "\n",
    "Now we have to build a query message and send it to OpenAI service. But before that we need to understand how to structure our query.\n",
    "\n",
    "<i> Chats with *OpenAI's gpt-3.5-turbo and gpt-4 chat models* are typically structured (in plain text) like this:</i>\n",
    "\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "User: Hi AI, how are you today?\n",
    "\n",
    "Assistant: I'm great thank you. How can I help you?\n",
    "\n",
    "User: I'd like to understand string theory.\n",
    "\n",
    "In the official OpenAI ChatCompletion endpoint these would be passed to the model in a format like:\n",
    "\n",
    "[\n",
    "    \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
    "    \n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "   \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step -3 </b>\n",
    "\n",
    "Send the formatted message to ChatGPT to get a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "response = llm_chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because response is just another AIMessage object, we can append it to messages, add another HumanMessage, and generate the next response in the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia Day is a national holiday in Australia that is celebrated annually on January 26th. It marks the anniversary of the arrival of the First Fleet at Port Jackson in New South Wales in 1788, which led to the establishment of the first European settlement in Australia.\n",
      "\n",
      "Australia Day is a day of national pride and celebration, with events and activities held across the country. It is a time for Australians to come together to reflect on the history of their nation, celebrate its diverse culture, and look towards the future.\n",
      "\n",
      "However, Australia Day is also a controversial holiday for many Indigenous Australians, as it marks the beginning of colonization and the negative impacts it had on their communities. Some Indigenous Australians refer to it as \"Invasion Day\" or \"Survival Day\" and use the day to raise awareness about the ongoing issues faced by Indigenous communities.\n",
      "\n",
      "Overall, Australia Day is a day of reflection, celebration, and debate in Australia, with different perspectives and opinions on its significance.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(response)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"I would like to know about Australia day\")\n",
    "\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "response = llm_chat.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional Work # 1** \n",
    "We can change the persona of ChatGPT. Such as provide output in French or any language other than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo! Mir geht es gut, danke. Wie kann ich Ihnen heute helfen?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates from english to german.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "]\n",
    "\n",
    "response = llm_chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional Work # 2** \n",
    "We can change the persona of ChatGPT. Such as enforcing OpenAI service to produce output in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"response\": \"I'm sorry, but I cannot provide information on how Napoleon was as it is a historical question. If you have any specific information or questions about Napoleon, feel free to ask and I'll do my best to assist you.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Preparing LangChain message prompt\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that parses json output.\"),\n",
    "    HumanMessage(content=\"How was Neploean ? Produce answer in json format\")\n",
    "]\n",
    "\n",
    "# Create the OpenAI object \n",
    "# The temperature value ranges from 0 to 2, with lower values indicating \n",
    "# greater determinism, and higher values indicating more randomness.\n",
    "llm_chat_json = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "response = llm_chat_json.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step -4 Dealing with Hallucinations </b>\n",
    "\n",
    "The knowledge of LLMs can be limited because LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. This knowledge is called the parametric knowledge of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "So, we expect to get hallucinated output from LLM if we ask about a more recent information. Such as enquiring about LLAMA 2 language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have information about a specific topic called \"Llama 2.\" Could you provide more context or details so I can better assist you?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"I want to know about Llama 2\"),\n",
    "]\n",
    "\n",
    "# send to chat-gpt\n",
    "response = llm_chat.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that OpenAI model failed to provide the output. To tackle this issue, we feed knowledge into LLMs in another way. It is called <b>source knowledge and it refers to any information fed into the LLM via the prompt</b>. We can do that as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle this issue, we feeding knowledge into LLMs in another way. It is called source knowledge and it refers to any information fed into the LLM via the prompt. We can try that with the Llama 2 question. We can take a description of this object from the Llama 2 source page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_information = [\n",
    "    \"Code Llama is a code generation model built on Llama 2, trained on 500B tokens of code. It supports common programming languages being used today, including Python, C++, Java, PHP, Typescript (Javascript), C#, and Bash.\",\n",
    "    \"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llama2_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed this additional information along with query to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"llama2\": {\n",
      "    \"description\": \"Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\",\n",
      "    \"features\": [\n",
      "      \"Supports common programming languages like Python, C++, Java, PHP, Typescript (Javascript), C#, and Bash.\",\n",
      "      \"Includes fine-tuned LLMs optimized for dialogue use cases, known as Llama 2-Chat.\",\n",
      "      \"Outperforms open-source chat models on most benchmarks tested.\",\n",
      "      \"Received positive feedback in human evaluations for helpfulness and safety.\",\n",
      "      \"Provides detailed descriptions of fine-tuning and safety improvements to encourage community contributions.\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you tell me about the llama 2 ?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\"\n",
    "\n",
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = llm_chat.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of this answer is phenomenal. This is made possible due to the augmention of our query with external knowledge (source knowledge). We can use the concept of vector databases to get this information automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building RAG Chatbots with LangChain\n",
    "\n",
    "In this example, we will build an AI chatbot from start-to-finish so that it can answer automatically about Llama 2 instead of providing the information manually (<b>source knowledge provided using vector database</b>). We will be using LangChain,HuggingFace embeddings, OpenAI, and vector DB, to build a chatbot capable of learning from the external world using Retrieval Augmented Generation (RAG).\n",
    "\n",
    "We will use two techniques to build our chatbot:\n",
    "\n",
    "1- Scrap a dataset sourced from the Llama 2 ArXiv paper and other related papers to help our chatbot answer questions about the latest and greatest in the world of GenAI.\n",
    "\n",
    "2- Scrap multiple webpages to help our chatbot answer questions about the latest and greatest in the world of GenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:red '> <b> Techniue 1 </b> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <span style='color:blue '>Importing the Data </span></b>\n",
    "\n",
    "We will import data. We will use the Hugging Face Datasets library to load our data. Specifically, we will be using the <b>\"jamescalam/llama-2-arxiv-papers\"</b> dataset.  \n",
    "\n",
    "The dataset we are using is sourced from the Llama 2 ArXiv papers. It is a collection of academic papers from ArXiv, a repository of electronic preprints approved for publication after moderation. Each entry in the dataset represents a \"chunk\" of text from these papers.\n",
    "\n",
    "Because most Large Language Models (LLMs) only contain knowledge of the world as it was during training, they cannot answer our questions about Llama 2 — at least not without this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doi': '1102.0183', 'chunk-id': '0', 'chunk': 'High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but', 'id': '1102.0183', 'title': 'High-Performance Neural Networks for Visual Object Classification', 'summary': 'We present a fast, fully parameterizable GPU implementation of Convolutional\\nNeural Network variants. Our feature extractors are neither carefully designed\\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\\narchitectures achieve the best published results on benchmarks for object\\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\\nback-propagation perform better than more shallow ones. Learning is\\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\\nrespectively.', 'source': 'http://arxiv.org/pdf/1102.0183', 'authors': ['Dan C. Cireşan', 'Ueli Meier', 'Jonathan Masci', 'Luca M. Gambardella', 'Jürgen Schmidhuber'], 'categories': ['cs.AI', 'cs.NE'], 'comment': '12 pages, 2 figures, 5 tables', 'journal_ref': None, 'primary_category': 'cs.AI', 'published': '20110201', 'updated': '20110201', 'references': []}\n",
      "4838\n"
     ]
    }
   ],
   "source": [
    "# Load dataset \n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\n",
    "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "    split=\"train\"\n",
    ")\n",
    "print(dataset[0])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <span style='color:blue '>1. Create a vector Database </span></b>\n",
    "\n",
    "We need to now store the dataset into a vector database such as chromadb.You can store data such as embeddings and text on the local computer. Perform query to get relevant documents and then pass it to online LLM model (such as OpenAI) to get a response.\n",
    "\n",
    "It is always a good idea to store embeddings which is more efficient to perform document similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Nabee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c3ec85d71d48c4838b3dd33e45445f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nabee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Nabee\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338607d4600644c5aaf060395a4eab6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd2d51224754fd08ec5d7320b230f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9eda9566b348ae86399699b822a9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nabee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6ff370c57447e3a1b3358bb52807f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dacdc12ca1b1400cb1f577c86d78311a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f97593fa3a4469d9f1a1ce2b76d25d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d6f486cb814cbc94847122f6a972e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984ef8d9e93048c8a9197532da5a247b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef15ab30e4b746759b032c2a5de15ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b73af354e4e474386f1fc4d35f315de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "\n",
    "# Create a hugging face transformation embedding model\n",
    "hugging_face_model = embedding_functions.SentenceTransformerEmbeddingFunction(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# Define a vector dataset location\n",
    "chroma_client = chromadb.PersistentClient(path=\"vetor_db/\")  # data stored in 'db' folder\n",
    "\n",
    "# Define a collection object\n",
    "#chroma_client.delete_collection(\"lang_chain_1\")\n",
    "db_collection = chroma_client.create_collection(name=\"lang_chain_1\",embedding_function=hugging_face_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example showing how we can use vector database tp get relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: id1\n",
      "Insert of existing embedding ID: id2\n",
      "Insert of existing embedding ID: id3\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['W24BZxF4pWkDiVagf05GEY44URozJBSfc+NtZ8iqSlmJKsOEkVihKdoTPqGCsSJ8tMieuYcGWXojCJpntBOof7eSBFXasZDM5kHVfNeLv7n9RM9OvdTKuJE51cVH40S5ujIyXtwhlQSrNnMEIQlNVkdPEESYW3aqpoSvPmTF0nnpOG5De/2tN68KOuowAEcwjF4cAZNuIYWtAHDAzzBC7xaj9az9Wa9/4wuWeXOPvyB9fENyBqUTg==</latexit><latexit']]\n"
     ]
    }
   ],
   "source": [
    "student_info = \"\"\"\n",
    "Alexandra Thompson, a 19-year-old computer science sophomore with a 3.7 GPA,\n",
    "is a member of the programming and chess clubs who enjoys pizza, swimming, and hiking\n",
    "in her free time in hopes of working at a tech company after graduating from the University of Washington.\n",
    "\"\"\"\n",
    "\n",
    "club_info = \"\"\"\n",
    "The university chess club provides an outlet for students to come together and enjoy playing\n",
    "the classic strategy game of chess. Members of all skill levels are welcome, from beginners learning\n",
    "the rules to experienced tournament players. The club typically meets a few times per week to play casual games,\n",
    "participate in tournaments, analyze famous chess matches, and improve members' skills.\n",
    "\"\"\"\n",
    "\n",
    "university_info = \"\"\"\n",
    "The University of Washington, founded in 1861 in Seattle, is a public research university\n",
    "with over 45,000 students across three campuses in Seattle, Tacoma, and Bothell.\n",
    "As the flagship institution of the six public universities in Washington state,\n",
    "UW encompasses over 500 buildings and 20 million square feet of space,\n",
    "including one of the largest library systems in the world.\"\"\"\n",
    "\n",
    "# Defining a collection\n",
    "db_collection.add(\n",
    "    documents = [student_info, club_info, university_info],\n",
    "    metadatas = [{\"source\": \"student info\"},{\"source\": \"club info\"},{'source':'university info'}],\n",
    "    ids = [\"id1\", \"id2\", \"id3\"]\n",
    ")\n",
    "\n",
    "# Query vector database\n",
    "results = db_collection.query(\n",
    "    query_texts=[\"What is the student name?\"],\n",
    "    n_results=1\n",
    ")\n",
    "print(results['documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <span style='color:blue '>2. Transform data and store in a vector database </span></b>\n",
    "\n",
    "Store each chunk of dataset with desired information such as metadata into vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59d89759d0d4b95be9ae5f74d6cba9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from chromadb.db.base import UniqueConstraintError\n",
    "\n",
    "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    doc_ids = [f\"{x['doi']}\" for i, x in batch.iterrows()]\n",
    "    # get text to embed\n",
    "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "    \n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['chunk'],\n",
    "         'source': x['source'],\n",
    "         'title': x['title']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Adding collections\n",
    "    db_collection.add(\n",
    "                documents= texts,\n",
    "                metadatas=metadata,\n",
    "                ids=ids\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve top 2 results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nabee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ricardo Lopez-Barquilla, Marc Shedroﬀ, Kelly Michelena, Allie Feinstein, Amit Sangani, Geeta\\nChauhan,ChesterHu,CharltonGholson,AnjaKomlenovic,EissaJamil,BrandonSpence,Azadeh\\nYazdan, Elisa Garcia Anzano, and Natascha Parks.\\n•ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philomena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organization support.\\n46\\n•Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\\nLlama team who helped get this work started.\\n•Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the ﬁgures in the\\npaper.\\n•Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\\ninternal demo.\\n•Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\\nLaurens van der Maaten, Jason Weston, and Omer Levy.', 'our responsible release strategy can be found in Section 5.3.\\nTheremainderofthispaperdescribesourpretrainingmethodology(Section2),ﬁne-tuningmethodology\\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\\nwork (Section 6), and conclusions (Section 7).\\n‡https://ai.meta.com/resources/models-and-libraries/llama/\\n§We are delaying the release of the 34B model due to a lack of time to suﬃciently red team.\\n¶https://ai.meta.com/llama\\n‖https://github.com/facebookresearch/llama\\n4\\nFigure 4: Training of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc : This process begins with the pretraining ofL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle using publicly\\navailableonlinesources. Followingthis,wecreateaninitialversionof L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc throughtheapplication']]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Create a hugging face model for embeddings\n",
    "model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf_embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_id,\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "# query the top 2 results\n",
    "query = 'What is so special about Llama 2'\n",
    "\n",
    "results = db_collection.query(\n",
    "    query_texts=query,\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "print(results['documents'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <span style='color:blue '>3. Implementing RAG </span></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function to extract source knowledge information from vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 5 results from knowledge base\n",
    "    results = db_collection.query(\n",
    "            query_texts=query,\n",
    "            n_results=3\n",
    "        )\n",
    "\n",
    "    source_knowledge= \"\"\n",
    "    # get the text from the results\n",
    "    for i in range(0, len(results['documents'])):\n",
    "        source_knowledge+= \"\\n\".join(results['documents'][i])\n",
    "    \n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    \n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now supplying additional information to OpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) that range in scale from 7 billion to 70 billion parameters. These models, known as Llama 2-Chat, are specifically optimized for dialogue use cases. They have demonstrated superior performance compared to open-source chat models across various benchmarks. Additionally, Llama 2-Chat has undergone human evaluations for helpfulness and safety, suggesting that it could potentially serve as a viable alternative to closed-source models. The developers of Llama 2 have provided detailed insights into their fine-tuning methodology and safety enhancements, aiming to facilitate further advancements in the responsible development of large language models.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = llm_chat.invoke(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue with more Llama 2 questions. Let's try without RAG first:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the development of Llama 2, several safety measures were implemented to ensure responsible usage of the large language models (LLMs). These safety measures included:\n",
      "\n",
      "1. **Human Evaluations**: Llama 2 underwent human evaluations for both helpfulness and safety. This process involved assessing the model's performance and ensuring it met certain safety standards.\n",
      "\n",
      "2. **Model Safety Optimization**: The Llama 2 models were optimized for dialogue use cases, and safety considerations were integrated into the fine-tuning process. This likely involved techniques to mitigate potential risks associated with language models, such as bias detection and mitigation.\n",
      "\n",
      "3. **Responsible Release Strategy**: The team behind Llama 2 had a responsible release strategy in place, as mentioned in Section 5.3 of the paper. This strategy likely involved guidelines for the ethical and safe deployment of the models.\n",
      "\n",
      "4. **Community Engagement**: The developers aimed to enable the community to build on their work and contribute to the responsible development of large language models. This collaborative approach can help address safety concerns and promote best practices in the field.\n",
      "\n",
      "Overall, the safety measures in the development of Llama 2 aimed to ensure that the models were not only effective in their performance but also ethically sound and safe for various applications.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"what safety measures were used in the development of llama 2?\"\n",
    ")\n",
    "\n",
    "res = llm_chat.invoke(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chatbot is able to respond about Llama 2 thanks to it's conversational history stored in messages. However, it doesn't know anything about the safety measures themselves as we have not provided it with that information via the RAG pipeline. Let's try again but with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The safety measures used in the development of Llama 2 include the following:\n",
      "\n",
      "1. Ethical Considerations and Limitations: The developers acknowledge that Llama 2 is a new technology that carries risks with its use. Testing conducted so far has been in English and may not cover all scenarios. Due to this, the potential outputs of Llama 2 cannot be predicted in advance, and the model may produce inaccurate or objectionable responses to user prompts.\n",
      "\n",
      "2. Responsible Use Guide: Developers are advised to perform safety testing and tuning tailored to their specific applications of Llama 2 before deploying any applications of the model. A Responsible Use Guide is available at https://ai.meta.com/llama/responsible-user-guide for reference.\n",
      "\n",
      "These safety measures aim to address the potential risks associated with the use of Llama 2 and emphasize the importance of responsible deployment and testing to ensure the model's outputs are accurate and appropriate for the intended applications.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"what safety measures were used in the development of llama 2?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = llm_chat.invoke(messages + [prompt])\n",
    "print(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
