{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building RAG Chatbots with LangChain\n",
    "In this example, we'll work on building an AI chatbot from start-to-finish. We will be using LangChain, OpenAI, and vector DB, to build a chatbot capable of learning from the external world using Retrieval Augmented Generation (RAG).\n",
    "\n",
    "We will be scrapping a using a dataset sourced from the Llama 2 ArXiv paper and other related papers to help our chatbot answer questions about the latest and greatest in the world of GenAI.\n",
    "\n",
    "By the end of the example we'll have a functioning chatbot and RAG pipeline that can hold a conversation and provide informative responses based on a knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot (no RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be relying heavily on the LangChain library to bring together the different components needed for our chatbot. To begin, we'll create a simple chatbot without any retrieval augmentation. We do this by initializing a ChatOpenAI object. For this we do need an OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the openAI key\n",
    "import getpass\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nabee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Creating an OpenAI object\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chats with *OpenAI's gpt-3.5-turbo and gpt-4 chat models* are typically structured (in plain text) like this:\n",
    "\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "User: Hi AI, how are you today?\n",
    "\n",
    "Assistant: I'm great thank you. How can I help you?\n",
    "\n",
    "User: I'd like to understand string theory.\n",
    "\n",
    "Assistant:\n",
    "The final \"Assistant:\" without a response is what would prompt the model to continue the conversation. In the official OpenAI ChatCompletion endpoint these would be passed to the model in a format like:\n",
    "\n",
    "\n",
    "\n",
    "[\n",
    "    \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
    "    \n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n",
    "\n",
    "]\n",
    "\n",
    "In LangChain there is a slightly different format. We use three message objects like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nabee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! As an AI, I don't have feelings, but I'm here to assist you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because res is just another AIMessage object, we can append it to messages, add another HumanMessage, and generate the next response in the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String theory is a theoretical framework in physics that aims to describe the fundamental building blocks of the universe. It suggests that instead of point-like particles, the fundamental objects are tiny, one-dimensional \"strings\" that vibrate in different modes.\n",
      "\n",
      "Here are some key points to understand about string theory:\n",
      "\n",
      "1. Dimensions: String theory requires the universe to have more than the usual three spatial dimensions (length, width, and height). It typically posits that there are extra spatial dimensions, usually six or seven in total, curled up and hidden at microscopic scales.\n",
      "\n",
      "2. Vibrating strings: In string theory, particles are not treated as point-like entities but as tiny strings. The way these strings vibrate determines their properties, such as mass and charge. Different vibrational patterns correspond to different particles.\n",
      "\n",
      "3. Unification of forces: One of the major goals of string theory is to unify all the fundamental forces of nature, including gravity, electromagnetism, and the strong and weak nuclear forces. By considering the vibrations and interactions of strings, it attempts to provide a unified framework for describing these forces.\n",
      "\n",
      "4. Quantum gravity: String theory also aims to reconcile quantum mechanics and general relativity, which are two fundamental theories that don't currently work together. General relativity describes gravity on a large scale, while quantum mechanics deals with the behavior of particles on a small scale. String theory incorporates both and offers a potential framework for understanding quantum gravity.\n",
      "\n",
      "5. Multiple versions: There are several different versions of string theory, such as Type I, Type IIA, Type IIB, heterotic SO(32), and heterotic E8Ã—E8. These versions describe different types of strings and various properties of the universe. Additionally, there are related theories like M-theory, which is thought to encompass all the different string theories and provide a more comprehensive framework.\n",
      "\n",
      "It's important to note that string theory is still a work in progress, and many aspects of it are still being actively researched. It holds promise for addressing some of the biggest questions in physics, but definitive experimental evidence is currently lacking. Nonetheless, it remains an intriguing and active area of study in theoretical physics.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"I would like to understand string theory\")\n",
    "\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Hallucinations\n",
    "\n",
    "The knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the parametric knowledge of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "The result of this is very clear when we ask LLMs about more recent information, like about the new (and very popular) Llama 2 LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 is a popular online game that has gained a lot of attention and popularity for several reasons. Here are a few special features of Llama 2:\n",
      "\n",
      "1. Engaging Gameplay: Llama 2 offers a unique and addictive gameplay experience. Players control a llama character and navigate through various obstacles and challenges. The game requires quick reflexes, strategy, and problem-solving skills to progress through different levels.\n",
      "\n",
      "2. Stunning Visuals: Llama 2 boasts impressive graphics and visually appealing designs. The game features vibrant colors, detailed environments, and cute character animations, creating an immersive and visually pleasing experience for players.\n",
      "\n",
      "3. Multiplayer Mode: Llama 2 offers a multiplayer mode where players can compete with their friends or other online players. This adds an extra layer of excitement and competitiveness to the game, allowing players to showcase their skills and challenge others.\n",
      "\n",
      "4. Regular Updates: The developers of Llama 2 are dedicated to providing a constantly evolving gaming experience. They frequently release updates, introducing new levels, challenges, and features to keep the game fresh and engaging for players.\n",
      "\n",
      "5. Social Integration: Llama 2 integrates social features, allowing players to connect with their friends, share their achievements, and compete on leaderboards. This enhances the social aspect of the game, making it more interactive and enjoyable.\n",
      "\n",
      "Overall, Llama 2 stands out for its engaging gameplay, stunning visuals, multiplayer mode, regular updates, and social integration, making it a special and highly popular online game.\n"
     ]
    }
   ],
   "source": [
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"What is so special about Llama 2?\"\n",
    ")\n",
    "\n",
    "# add to messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "]\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the model answer is totally wrong. Llama 2 is not a game. Lets try model response for LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Can you tell me about the LLMChain in LangChain?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but I'm not familiar with a specific entity called \"LLMChain\" in the context of LangChain. It's possible that it could be a term or acronym specific to the LangChain platform or project. However, without more information, I'm unable to provide specific details about LLMChain. It would be best to refer to official documentation, whitepapers, or announcements from LangChain to get accurate information about LLMChain or reach out to their support team for clarification.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear from this answer that the LLM doesn't know the informaiton, but sometimes an LLM may respond like it does know the answer â€” and this can be very hard to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding source knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way of feeding knowledge into LLMs. It is called source knowledge and it refers to any information fed into the LLM via the prompt. We can try that with the LLMChain question. We can take a description of this object from the LangChain documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now feed this information to our chat bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMChain is a type of chain within the LangChain framework. It is the most common type of chain and is used to connect a language model (either an LLM or a ChatModel) to other components within an application. \n",
      "\n",
      "The LLMChain consists of three main components: a PromptTemplate, a model, and an optional output parser. The PromptTemplate is responsible for formatting multiple input variables into a suitable prompt. This formatted prompt is then passed to the language model (LLM or ChatModel) within the chain. \n",
      "\n",
      "Once the model receives the prompt, it generates an output based on the provided input. The LLMChain also allows for the inclusion of an OutputParser, which can be used to parse and format the output of the language model into a final desired format.\n",
      "\n",
      "Overall, the LLMChain plays a crucial role in enabling applications powered by language models within the LangChain framework. It facilitates the integration of language models with other data sources and allows for interactions between the language model and its environment.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of this answer is phenomenal. This is made possible due to the augmention of our query with external knowledge (source knowledge). We can use the concept of vector databases to get this information automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Data\n",
    "\n",
    "In this task, we will be importing our data. We will be using the Hugging Face Datasets library to load our data. Specifically, we will be using the <b>\"jamescalam/llama-2-arxiv-papers\"</b> dataset. This dataset contains a collection of ArXiv papers which will serve as the external knowledge base for our chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489539f240c34d7ba77ec9d3cb9ca287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/409 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451332f65f9449888118ab1b8dcb05b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c530aef033243e7868a702f209583c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\n",
    "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "    split=\"train\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '1102.0183',\n",
       " 'chunk-id': '2',\n",
       " 'chunk': 'promising architectures for such tasks. The most successful hierarchical object recognition systems\\nall extract localized features from input images, convolving image patches with \\x0clters. Filter\\nresponses are then repeatedly sub-sampled and re-\\x0cltered, resulting in a deep feed-forward network\\narchitecture whose output feature vectors are eventually classi\\x0ced. One of the \\x0crst hierarchical\\nneural systems was the Neocognitron (Fukushima, 1980) which inspired many of the more recent\\nvariants.\\nUnsupervised learning methods applied to patches of natural images tend to produce localized\\n\\x0clters that resemble o\\x0b-center-on-surround \\x0clters, orientation-sensitive bar detectors, Gabor \\x0clters\\n(Schmidhuber et al. , 1996; Olshausen and Field, 1997; Hoyer and Hyv\\x7f arinen, 2000). These \\x0cndings\\nin conjunction with experimental studies of the visual cortex justify the use of such \\x0clters in the\\nso-called standard model for object recognition (Riesenhuber and Poggio, 1999; Serre et al. , 2007;\\nMutch and Lowe, 2008), whose \\x0clters are \\x0cxed, in contrast to those of Convolutional Neural',\n",
       " 'id': '1102.0183',\n",
       " 'title': 'High-Performance Neural Networks for Visual Object Classification',\n",
       " 'summary': 'We present a fast, fully parameterizable GPU implementation of Convolutional\\nNeural Network variants. Our feature extractors are neither carefully designed\\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\\narchitectures achieve the best published results on benchmarks for object\\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\\nback-propagation perform better than more shallow ones. Learning is\\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\\nrespectively.',\n",
       " 'source': 'http://arxiv.org/pdf/1102.0183',\n",
       " 'authors': ['Dan C. CireÅŸan',\n",
       "  'Ueli Meier',\n",
       "  'Jonathan Masci',\n",
       "  'Luca M. Gambardella',\n",
       "  'JÃ¼rgen Schmidhuber'],\n",
       " 'categories': ['cs.AI', 'cs.NE'],\n",
       " 'comment': '12 pages, 2 figures, 5 tables',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.AI',\n",
       " 'published': '20110201',\n",
       " 'updated': '20110201',\n",
       " 'references': []}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Overview\n",
    "\n",
    "The dataset we are using is sourced from the Llama 2 ArXiv papers. It is a collection of academic papers from ArXiv, a repository of electronic preprints approved for publication after moderation. Each entry in the dataset represents a \"chunk\" of text from these papers.\n",
    "\n",
    "Because most Large Language Models (LLMs) only contain knowledge of the world as it was during training, they cannot answer our questions about Llama 2 â€” at least not without this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Knowledge Base\n",
    "\n",
    "We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using HuggingFace model to generate embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 384)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_id,\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'my name is xyz'\n",
    "]\n",
    "\n",
    "res = hf_embedding.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.011914796195924282, 0.08918038755655289, 0.0382428839802742, 0.01908116601407528, 0.05977559834718704, 0.0053163692355155945, 0.038781408220529556, -0.008798947557806969, 0.060234904289245605, -0.015470288693904877, 0.027178632095456123, 0.05887822061777115, -0.02538408152759075, -0.033832449465990067, -0.019123584032058716, -0.0002414310147287324, 0.06661207228899002, -0.06140352413058281, -0.00425687013193965, -0.0020314722787588835, 0.02755112014710903, 0.13663724064826965, 0.0045464178547263145, 0.02143850550055504, 0.04543565586209297, 0.07764163613319397, -0.09471825510263443, 0.0631301999092102, 0.06228697672486305, -0.03511490300297737, -0.00600011320784688, -0.029994618147611618, 0.155880868434906, 0.05987439304590225, 0.01639336161315441, 0.04131562262773514, 0.008261461742222309, 0.028696317225694656, 0.02570163644850254, 0.05118108168244362, 0.03644575923681259, -0.11936520785093307, -0.008073659613728523, 0.05804252624511719, 0.01930527202785015, -0.005999195855110884, -0.032330472022295, 0.022044692188501358, 0.05056128650903702, -0.04238983616232872, -0.057113781571388245, -0.02587663009762764, -0.01899614743888378, 0.04339314624667168, 0.07063183933496475, 0.04593487083911896, -0.0011733636492863297, 0.0008359960047528148, -0.02069826051592827, 0.01082269661128521, -0.023847293108701706, 0.04672970622777939, -0.011276517063379288, 0.023529257625341415, 0.10439988225698471, -0.04034710302948952, -0.008016739040613174, -0.0179460309445858, -0.11190194636583328, 0.058692578226327896, -0.019183777272701263, 0.04420263692736626, -0.02434849925339222, 0.08135545998811722, -0.07509398460388184, 0.018027227371931076, -0.028708836063742638, -0.020068464800715446, 0.03861797973513603, 0.04260294511914253, -0.05404843017458916, -0.04819589480757713, 0.05432971939444542, 0.05254717171192169, -0.11023948341608047, 0.04403562843799591, 0.0012400192208588123, -0.039668962359428406, -0.0017445951234549284, -0.020906804129481316, -0.008455004543066025, -0.06290605664253235, 0.09616648405790329, 0.03390558063983917, -0.08172976225614548, 0.05188392102718353, -0.0931609496474266, -0.07571720331907272, 0.028836624696850777, 0.10810822248458862, -0.011175212450325489, 0.007147559430450201, 0.10201504081487656, 0.01504968199878931, -0.10165291279554367, -0.14965015649795532, 0.0016647785669192672, 0.018260635435581207, -0.015966134145855904, 0.00709972670301795, -0.0014102464774623513, -0.06131304055452347, -0.061759356409311295, -0.04385090246796608, 0.012018860317766666, -0.05591582879424095, 0.03730334714055061, 0.0356956422328949, 0.054854463785886765, 0.095671147108078, 0.013410900719463825, -0.02999228984117508, -0.1101694405078888, 0.020531879737973213, -0.019637800753116608, -0.09004488587379456, 0.060066189616918564, -5.157916397142591e-33, -0.01607365719974041, 0.02226310409605503, 0.007580702658742666, 0.07484086602926254, -0.008172893896698952, 0.034861717373132706, -0.05972011014819145, -0.008400238119065762, -0.0659836009144783, -0.042051710188388824, -0.01327063050121069, -0.07245957106351852, 0.029557550325989723, 0.018043436110019684, -0.04595625400543213, -0.0911373719573021, -0.08147478848695755, 0.07296457886695862, -0.029998155310750008, -0.004367897752672434, -0.09750495105981827, 0.006048627197742462, 0.0533333383500576, -0.039105743169784546, -0.05878903344273567, -0.03476154804229736, 0.022434227168560028, -0.03049047477543354, -0.027194876223802567, 0.015396841801702976, -0.09561193734407425, 0.017127174884080887, -0.0349152646958828, -0.08179987221956253, 0.02261645719408989, -0.01694643683731556, 0.03350752219557762, -0.0555974505841732, -0.06227726861834526, 0.00025476948940195143, -0.020174909383058548, 0.01797044835984707, 0.03305772319436073, -0.06777242571115494, -0.05925353243947029, 0.036866214126348495, -0.06011708453297615, 0.017863566055893898, 0.03466200456023216, 0.0408579483628273, 0.002920771948993206, 0.06043170392513275, 0.0851440504193306, 0.0015096744755282998, 0.025944093242287636, -0.030597444623708725, 0.000983489560894668, -0.0016708442708477378, -0.010750113986432552, 0.03168049827218056, -0.027169102802872658, 0.010845216922461987, 0.007694669533520937, 0.022395750507712364, 0.012174401432275772, 0.02323012799024582, 0.0012213386362418532, 0.008176135830581188, 0.044049594551324844, -0.042946599423885345, -0.0871000662446022, -0.028252046555280685, 0.019847322255373, 0.011975427158176899, -0.028659477829933167, 0.01384666096419096, 0.045781999826431274, -0.017643015831708908, -0.03612826019525528, 0.0022757414262741804, -0.03952036052942276, -0.05773027241230011, 0.0457979254424572, -0.027737285941839218, -0.04771813750267029, 0.028791917487978935, 0.06926874816417694, -0.10445984452962875, 0.008511356078088284, 0.033091504126787186, -0.11496436595916748, -0.009323406964540482, 0.05813830345869064, -0.019148878753185272, 0.03861051797866821, 3.4917486637680025e-33, 0.008932754397392273, -0.043765340000391006, -0.024995988234877586, -0.04625536501407623, 0.05353677645325661, 0.03627536818385124, 0.015142596326768398, 0.12704896926879883, 0.05500338599085808, 0.11018926650285721, 0.02007696032524109, 0.05492710322141647, 0.004946630448102951, -0.03982638940215111, -0.00825713761150837, 0.018428640440106392, 0.14006979763507843, -0.01614900678396225, 0.04169086366891861, 0.04481018707156181, -1.7136202586698346e-05, -0.053245749324560165, -0.10418049991130829, 0.06364789605140686, 0.020523544400930405, 0.06157074496150017, 0.06816276907920837, -0.005195443984121084, -0.033843815326690674, 0.006742042023688555, -0.01383448950946331, -0.06437095999717712, -0.02684052661061287, -0.03676151856780052, -0.06195804849267006, 0.002532744314521551, 0.011683684773743153, 0.003753715893253684, 0.016322994604706764, 0.035931192338466644, 0.1082378551363945, 0.0063264803029596806, -0.02866494469344616, -0.007885164581239223, -0.02155902236700058, -0.11726841330528259, -0.011050323955714703, 0.013491887599229813, -0.04297732934355736, 0.044990792870521545, -0.018759477883577347, 0.005730228032916784, -0.03233309090137482, -0.027176853269338608, -0.014903615228831768, -0.0014459339436143637, -0.046752311289310455, 0.053703706711530685, -0.013810439966619015, -0.062006089836359024, -0.059908874332904816, 0.08883471041917801, -0.0833938717842102, 0.022770151495933533, 0.12534591555595398, -0.06910187005996704, -0.00197196495719254, 0.046112991869449615, -0.03095337562263012, -0.017357585951685905, 0.012061415240168571, 0.00612834794446826, -0.10325513780117035, -0.04677514731884003, 0.003460249165073037, 0.010174361988902092, 0.03646330162882805, -0.01863963156938553, -0.044253747910261154, -0.012084798887372017, 0.09426414966583252, 0.01080222986638546, -0.0281914584338665, -0.004151984583586454, 0.04992157593369484, 0.023514512926340103, -0.050248172134160995, -0.039618831127882004, -0.04073620215058327, 0.02161136083304882, -0.11234959214925766, 0.0012425117893144488, 0.08704547584056854, 0.14981691539287567, -0.08597774058580399, -1.8664652046140873e-08, 0.02176903747022152, -0.08333982527256012, -0.10831104964017868, 0.041553251445293427, 0.019398586824536324, 0.10349943488836288, 0.01819884404540062, -0.048211757093667984, -0.0405060276389122, -0.032348524779081345, 0.037668582051992416, 0.06856855005025864, -0.03812140226364136, -0.04512752220034599, 0.0017980297561734915, 0.007001540623605251, -0.012798693031072617, -0.04065538942813873, -0.03363972529768944, 9.859518468147144e-05, 0.04512248933315277, 0.029293891042470932, -0.02195027843117714, -0.0758577361702919, -0.0014653666876256466, 0.02224637381732464, -0.031352266669273376, 0.04836314544081688, -0.000987336621619761, -0.026937980204820633, 0.017278259620070457, 0.09258858859539032, -0.034599289298057556, -0.02310839295387268, 0.023140275850892067, 0.03397297114133835, 0.03020413964986801, 0.018593208864331245, 0.05794735625386238, -0.06191714107990265, 0.04107508063316345, -0.03941791504621506, 0.041510529816150665, 0.035336703062057495, -0.047244008630514145, 0.035490673035383224, -0.006324863061308861, -0.020437046885490417, 0.025205377489328384, -0.10108444094657898, -0.008422926999628544, 0.003546674968674779, 0.04185090959072113, 0.022386664524674416, -0.004537032917141914, -0.012016880325973034, 0.03693045303225517, 0.02943968027830124, -0.07024739682674408, 0.039330486208200455, 0.05964121222496033, 0.10893730819225311, 0.11683540046215057, 0.003669008379802108]\n"
     ]
    }
   ],
   "source": [
    "print(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we get two (aligning to our two chunks of text) 384-dimensional embeddings.\n",
    "\n",
    "We're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Chroma' has no attribute 'Collection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25484\\3505387098.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChroma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mChroma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCollection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"my_collection\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Chroma' has no attribute 'Collection'"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Define a vector data base client\n",
    "chroma_client = chromadb.Client()\n",
    "client.delete_collection(name=\"chroma_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca0089b6cea4aaabc365c4ffc7bf538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "\n",
    "\n",
    "# Define a vector data base client\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"my_collections\")\n",
    "\n",
    "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
    "\n",
    "batch_size = 100\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    # get text to embed\n",
    "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = hf_embedding.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['chunk'],\n",
    "         'source': x['source'],\n",
    "         'title': x['title']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Adding collections\n",
    "    collection.add(\n",
    "                metadatas=metadata,\n",
    "                embeddings=embeds,\n",
    "                ids=ids\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a vector database on a local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4838\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "vectorstore = Chroma(client=chroma_client, collection_name=\"my_collections\") \n",
    "\n",
    "print(vectorstore._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation\n",
    "We've built a fully-fledged knowledge base. Now it's time to connect that knowledge base to our chatbot. To do that we'll be diving back into LangChain and reusing our template prompt from earlier.\n",
    "\n",
    "To use LangChain here we need to load the LangChain abstraction for a vector index, called a vectorstore. We pass in our vector index to initialize the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
